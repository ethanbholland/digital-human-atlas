## 2025

### May 2025

- https://microsoft.github.io/GASP/
- GASP
- https://www.dell.com/en-us/blog/edge-ai-innovation-real-time-pose-detection/
- Edge AI Innovation: Real-Time Pose Detection | Dell
- https://ziqiaopeng.github.io/synctalk++/
- SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting
- https://alignhuman.github.io/
- AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation
- https://x.com/NandoDF/status/1933266267663634465
- Nando de Freitas on X: "RT @_akhaliq: ByteDance presents APT2 Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation https://t.co/yBD‚Ä¶" / X
- We just launched our biggest update yet. Meet Higgsfield Speak ‚Äî the fastest way to make motion-driven talking videos. Pick a style, choose an avatar, type a script. We do the rest ‚Äî cinematic motion, voice, emotion. Comment Speak to get the full guide + promo code in the DM. https://x.com/higgsfield_ai/status/1930686472845455417
- Voice cloning is now trivially easy with open source tools, while live avatar videos of real people are easy with proprietary tools &amp; a variety of open source tools are getting there. Very limited time to adjust legal &amp; financial safeguards to new ways of authenticating people"" / X https://x.com/emollick/status/1931364236304830675
- https://www.meta.com/blog/instagram-3d-photos-spatial-medianavigator-meta-quest-test/
- Social Goes Spatial & the Beginning of Our Reimagined OS | Meta Quest Blog | Meta Store
- https://x.com/ID_AA_Carmack/status/1933199948759146810
- John Carmack on X: "The beta 3D photo integration with Instagram is very well done! Every static photo becomes an AI generated stereoscopic 3D photo, and there is a ‚Äú3D‚Äù button that lets you toggle the feature on and off for comparison. Every photo I looked at ‚Äújust worked‚Äù, with no glaring" / X
- HERE WE GO
- https://www.captions.ai/blog-post/introducing-mirage-studio
- Discover Mirage Studio ‚Äî the fastest way to generate studio-quality videos with lifelike AI actors. Create high-performing content at scale without cameras, crews, or production delays.
- Introducing Mirage Studio
- https://x.com/bilawalsidhu/status/1929670896416895125
- Tangentially related ‚ÄúWorld Labs open sourced their 3d gaussian splat renderer; built as a first-class ThreeJS citizen instead of bolting it on. Multiple splats, proper scene hierarchy, can transform and animate them, mix with regular meshes, plus works on mobile and VR.‚Äù
- Monocular pose estimation has gotten really good Grab any 2D video and transfer the performance to a 3D character https://x.com/bilawalsidhu/status/1928612111896174870
- These wild street interviews have completely taken over my feed. AI video had hit a quality bar already, but Veo 3 with native audio output has unlocked whole new category of creators (and thus content). Ppl who would've never bothered duct taping multiple tools together can  https://x.com/bilawalsidhu/status/1929568408820949350
- Will Veo 3 eat all of the above links?
- https://x.com/getcaptionsapp/status/1929554635544461727
- Introducing Mirage Studio. Powered by our proprietary omni-modal foundation model. Generate expressive videos at scale, with actors that actually look and feel alive. Our actors laugh, flinch, sing, rap ‚Äî all of course, per your direction.
- https://x.com/TencentHunyuan/status/1927575170710974560
- Introducing HunyuanVideo-Avatar, a model jointly developed by Tencent Hunyuan and Tencent Music, bringing photos to life.
- https://www.heygen.com/blog/introducing-ai-studio
- HeyGen AI Studio
- https://x.com/EHuanglu/status/1919696421625987220
- HeyGen just dropped Voice Mirroring, it can clone anyone's voice emotion, tone and style PERFECTLY
- JUNE 2026
- https://kkakkkka.github.io/HunyuanPortrait/
- HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation
- https://ziqiaopeng.github.io/OmniSync/
- OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers
- https://chathuman.github.io/
- ChatHuman
- https://haoningwu3639.github.io/SpatialScore/
- SpatialScore (tangential)
- https://starc52.github.io/publications/2025-05-28-LoRAvatar/
- LoRAvatar
- https://kebii.github.io/DreamDance/
- DreamDance
- https://simongiebenhain.github.io/pixel3dmm/
- Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction
- https://hunyuanvideo-avatar.github.io/
- HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters
- https://jonathsch.github.io/becominglit/
- BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading
- https://ziqiaopeng.github.io/dualtalk/
- DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations
- https://shenhanqian.github.io/gaussian-avatars
- GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians | Shenhan Qian
- CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesishttps://fraunhoferhhi.github.io/cgs-gan/
- What if you could take a 2D video call and make it feel like you‚Äôre really there? Google Beam, our new AI-first video communication platform, does just that ‚Äî using a state-of-the-art AI video model to transform 2D video streams into a realistic 3D experience. #GoogleIO https://x.com/Google/status/1924875328037466302
- New sota open-source depth estimation: Marigold IID üåº &gt; normal maps, depth maps of scenes &amp; faces &gt; get albedo (true color) and BRDF (texture) maps of scenes, they even release a depth-to-3D printer format demo üòÆ link to all models and demos on the next one ‚§µÔ∏è https://x.com/mervenoyann/status/1923318140965990814
- Depthing
- https://vcai.mpi-inf.mpg.de/projects/EVA/
- EVA: Expressive Virtual Avatars from Multi-view Videos
- It's fun to see all the new uses cases coming up with this latest References update. Feels like Christmas but every day. Here is zero-shot novel view synthesis for people and characters, works straight out of the box. https://x.com/c_valenzuelab/status/1922656353354412332
- üé• @higgsfield_ai Hollywood-Level Videos from a Single Image Uses 50+ pro-level camera moves ‚Äî from bullet time to crash zooms, robo arms, and FPV chases ‚Äî to turn static images into cinematic videos Some beutiful examples.. üßµ 1/n - 3D Rotation The subject or product spins https://x.com/rohanpaul_ai/status/1922241875089543546
- Interesting fork in the road‚Ä¶ do I start including image to video?  I‚Äôve ignored it so far (as it pertains to this tech)... but this feels very ‚ÄúViggle‚Äù ish‚Ä¶
- TeGA: Texture Space Gaussian Avatars for High-Resolution DynamicHead Modeling https://syntec-research.github.io/UVGA/
- https://huggingface.co/collections/prs-eth/marigold-computer-vision-6669e9e3d3ee30f48214b9ba
- Marigold Computer Vision - a prs-eth Collection
- New sota open-source depth estimation: Marigold IID üåº &gt; normal maps, depth maps of scenes &amp; faces &gt; get albedo (true color) and BRDF (texture) maps of scenes, they even release a depth-to-3D printer format demo üòÆ link to all models and demos on the next one ‚§µÔ∏è  https://x.com/mervenoyann/status/1923318140965990814
- https://eastbeanzhang.github.io/GUAVA/
- GUAVA - Project page - GUAVA: Generalizable Upper Body 3D Gaussian Avatar
- https://mattie-e.github.io/Co3/
- Co3 - Co Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion
- https://seva100.github.io/GeomHair
- GeomHair- Reconstruction of Hair Strands from Colorless 3D Scans

### March 2025

- Character.AI unveils AvatarFX, an AI video model to create lifelike chatbots | TechCrunch https://techcrunch.com/2025/04/22/character-ai-unveils-avatarfx-an-ai-video-model-to-create-lifelike-chatbots/
- https://x.com/rowancheung/status/1917095368073830652
- Rowan Cheung on X: "Higgsfield AI introduced Iconic Scenes! The feature re-creates famous movie scenes with a different subject using just a single selfie from the user Available to try on the Higgsfield AI website https://t.co/EqaMiW4DPL" / X
- Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction https://simongiebenhain.github.io/pixel3dmm/
- https://antonibigata.github.io/KeySync/
- KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution
- https://y-u-a-n-l-i.github.io/projects/IM-Portrait/
- IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos
- "Alibaba just announced Uni3C on Hugging Face Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation https://x.com/_akhaliq/status/1914619143925432338
- "We just dropped a new SoTA lipsync model on @FAL: Hummingbird-0 Available now as a research preview, it's the most accurate zero-shot lipsync model we‚Äôve tested, open or closed source. https://x.com/heytavus/status/1915435703833641231
- "My AI avatar just hit 100,000 followers. The videos you see below aren't actually me, it's an AI avatar of my face/voice. Despite this, each of these videos went viral with millions of views across platforms. Some raw thoughts on this experiment: 1) People don't care it's https://x.com/rowancheung/status/1914706445817233507
- "California-based Play AI released an AI voice changer The tool allows users to change their voice into ANYONE else's with just 10 seconds of audio Accessible on mobile, it even preserves the same voice and tone of the original recording! https://x.com/rowancheung/status/1914567423916638560
- "Character AI unveiled an AI platform called AvatarFX It allows users to create long-form, coherent videos of talking avatars from just a single reference photo and voice selection Launching soon to CAI+ subscribers! https://x.com/rowancheung/status/1914930092632928655
- "HeyGen is using gpt-image-1 to enhance avatar creation, specifically improving avatar editing within the platform. https://x.com/OpenAIDevs/status/1915097077878530334
- AvatarFX is a cutting-edge AI platform for interactive storytelling, empowering users to bring characters to life by simply uploading an image and selecting a voice.  https://character-ai.github.io/avatar-fx/
- https://sega-head.github.io/
- SEGA: Drivable 3D Gaussian Head Avatar from a Single Image
- https://simonwillison.net/2025/Apr/18/gemini-image-segmentation/
- Image segmentation using Gemini 2.5
- https://huggingface.co/spaces/InstantX/InstantCharacter
- InstantCharacter - a Hugging Face Space by InstantX
- https://x.com/XRarchitect/status/1909730663877800030
- I‚ñ≤N CURTIS on X: "Face tracking + 3D model morph targets Built with Bolt Rendered Using Three.js Powered by MediaPipe Try the demo here: https://t.co/y273B9tFDY Bolt's new mascot maybe üòÖ clone the project -&gt; https://t.co/DOm5CHnlFf" / X
- https://face-landmark-tracking.netlify.app/
- MediaPipe Face Landmark
- https://nlml.github.io/sheap/
- SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians
- https://jzr99.github.io/DNF-Avatar/
- DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting
- https://fantasy-amap.github.io/fantasy-talking/
- FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis
- https://visinf.github.io/cups/
- Scene-Centric Unsupervised Panoptic Segmentation
- "Alibaba just released LAM on Hugging Face Large Avatar Model for One-shot Animatable Gaussian Head https://x.com/_akhaliq/status/1910259092972589432
- "@AgentOpsAI 13/ Create Multilingual 2D Digital Humans for Enterprise Hosted by: Rochelle Pereira, Sr. Director of Engineering, NVIDIA Ragav Venkatesan, Principal Software Engineer, NVIDIA Learn about NVIDIA NIMƒÅ‚Äû¬¢ microservices for secure, high-performance AI deployment across various" / X https://x.com/AtomSilverman/status/1907898487154626722
- https://cv.maxi.su/DiTaiListener/
- DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion
- https://a16z.com/ai-avatars/
- AI Avatars Escape the Uncanny Valley | Andreessen Horowitz
- How Tavus is helping to make AI videos feel like real conversations https://ai.meta.com/blog/tavus-real-feeling-ai-videos-llama/
- OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication https://humanaigc.github.io/omnitalker/
- "Meta just announced MoCha, an AI model that produces movie-grade talking/singing character animations from speech and text It even enables multi-character conversations with turn-based dialogue generation https://x.com/rowancheung/status/1907304629060198611
- MoCha: Towards Movie-Grade Talking Character Synthesis https://congwei1230.github.io/MoCha/
- Deep Imagination Research | NVIDIA https://research.nvidia.com/labs/dir/akd/
- DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness https://ruiningli.com/dso
- DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance https://grisoon.github.io/DreamActor-M1/
- "Reconstructing dynamic 3D scenes (4D geometry) from single videos struggles with motion and often requires complex methods or specialized data. Sora3R solves this by leveraging large video diffusion models' learned understanding of motion and structure to directly generate 4D https://x.com/rohanpaul_ai/status/1906536678032109698
- Segment Any Motion in Videos https://motion-seg.github.io/
- with Hybrid Guidance https://arxiv.org/pdf/2504.01724
- DreamActor-M1: Holistic, Expressive and Robust Human Image Animation
- MoCha: Towards Movie-Grade Talking Character Synthesis  https://arxiv.org/pdf/2503.23307v1
- APRIL 2025
- https://huggingface.co/ByteDance/InfiniteYou
- ByteDance/InfiniteYou ¬∑ Hugging Face
- KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation https://antonibigata.github.io/KeyFace/
- HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation https://kkakkkka.github.io/HunyuanPortrait/
- https://kh129.github.io/hogs/
- HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting
- Roblox‚Äôs new AI model can generate 3D objects | The Verge https://www.theverge.com/news/630977/roblox-cube-3d-objects-mesh-ai-text-prompt
- https://x.com/TXhunyuan/status/1902195164182888775
- Hunyuan on X: "We are thrilled to announce a ‚Äã30x acceleration in white model generation speed across the entire Hunyuan3D 2.0 family, reducing the processing time from 30 seconds to just ‚Äã1 second. https://t.co/wYWSKVvj2e" / X
- https://x.com/bilawalsidhu/status/1902518941580234891
- Bilawal Sidhu on X: "The ‚Äúmetaverse‚Äù is a rather empty place without 3d content ‚Äî generative AI gives us the means to populate it. Here‚Äôs a few things that have blown my mind lately ‚Äî and why I think Roblox might be the perfect sandbox for these primitives coming together. https://t.co/xdev08JMkH" / X
- https://kaldir.vc.in.tum.de/nersemble_benchmark/
- NeRSemble Benchmark
- https://kaldir.vc.in.tum.de/nersemble_benchmark/
- NeRSemble Benchmark
- https://pixelai-team.github.io/TaoAvatar/
- TaoAvatar
- https://huggingface.co/papers/2503.10625
- Paper page - LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds
- https://jianhongbai.github.io/ReCamMaster/
- ReCamMaster: Camera-Controlled Generative Rendering from A Single Video
- https://imagintalk.github.io/
- ImaginTalk
- https://zhenglinzhou.github.io/Zero-1-to-A/
- HeadStudio
- https://susunghong.github.io/MusicInfuser/
- MusicInfuser: Making Video Diffusion Listen and Dance
- https://freedomgu.github.io/DiffPortrait360/
- DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis
- https://silence-tang.github.io/gaussian-ip/
- GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior
- https://gapszju.github.io/RGBAvatar/
- RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars
- https://x.com/rowancheung/status/1902250078913360345
- Rowan Cheung on X: "Stability AI unveiled Stable Virtual Camera, a new diffusion model It transforms single images into 3D videos with 14 dynamic camera paths Currently in research preview under a non-commercial license! https://t.co/MKNPlA1P4m" / X
- https://x.com/_akhaliq/status/1902560381370839524
- AK on X: "Roblox just released Cube on Hugging Face A Roblox View of 3D Intelligence https://t.co/yMwmmlmrVU" /
- https://marcb.pro/atu/
- Animating the Uncaptured
- https://x.com/_akhaliq/status/1902199977096499424
- AK on X: "Tencent just announced a ‚Äã30x acceleration in model generation speed across the entire Hunyuan3D 2.0 family, reducing the processing time from 30 seconds to just ‚Äã1 second, available on Hugging Face https://t.co/9BwipKQKR7" / X
- AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models https://kwanyun.github.io/AnyMoLe_page/
- https://x.com/MoveAI_/status/1899489539380617454
- Move AI on X: "Introducing Gen 2 spatial motion: 3D motion capture, full-body dynamics, joint torques, ground reaction forces, advanced motion retargeting, motion prediction. Works on AI video, phones, cinema cams, stadia. Launched at Movement Day, BAFTA London. https://t.co/2PKRJhzbZO" / X
- https://steve-zeyu-zhang.github.io/MotionAnything/
- Motion Anything
- https://magicinfinite.github.io/
- MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice
- https://huggingface.co/motimalu/wan-flat-color-v2
- motimalu/wan-flat-color-v2 ¬∑ Hugging Face
- https://huggingface.co/Remade-AI/Rotate
- Remade-AI/Rotate ¬∑ Hugging Face

### February 2025

- https://xuanchenli.github.io/TexTalk/
- Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture
- https://uco3d.github.io/
- UnCommon Objects in 3D
- https://ltt-o.github.io/Kiss3dgen.github.io/
- Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation
- https://fictionarry.github.io/InsTaG/
- InsTaG: Learning Personalized 3D Talking Head from Few-Second Video
- "This could be huge for AI storytelling. I've tried so many AI lipsync tools and have never been satisfied. This is my first try with @hedra_labs Character-3 and it's blown me away. It was so easy. I uploaded a track from @SunoMusic and an image from @midjourney and it just https://x.com/TomLikesRobots/status/1898009257598980587
- https://genesisorigin.github.io/
- BodyGen: Advancing Towards Efficient Embodiment Co-Design
- Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars https://tobias-kirschstein.github.io/avat3r/
- "Awesome research from ByteDance continues. Current methods of Subject-to-video merges text prompts and reference images to produce consistent videos, yet many approaches fail to preserve subject fidelity. This new research Phantom merges text and reference-image features in a  https://x.com/rohanpaul_ai/status/1894000198210490440
- https://sites.google.com/view/cast4
- CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image
- NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis https://nerf-3dtalker.github.io/NeRF-3Dtalker/
- https://x.com/skalskip92/status/1892497124534747193
- SkalskiP on X: ""new" SOTA object detector, and it's NOT YOLOv12 D-FINE is a model released 3 months ago under Apache-2.0 license; I have no idea how it flew under my radar @onuralpszr thanks for adding it to leaderboard leaderboard link: https://t.co/9z9ImuoChZ ‚Üì more about architecture https://t.co/Q8i4aURQ4n" / X
- "The internet going wild with the microwave AI filter -- prolly because it's pure nightmare fuel üò≠ https://x.com/bilawalsidhu/status/1892789671672918425
- https://huggingface.co/papers/2502.19204
- Paper page - Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator
- https://x.com/_akhaliq/status/1892073250974216476
- AK on X: "ByteDance presents Phantom Subject-consistent video generation via cross-modal alignment https://t.co/9dV7QA0DD6" / X
- https://x.com/minchoi/status/1890074266244395495
- Min Choi on X: "2. Goku+: Product and Human Interaction https://t.co/KGtK4DPxTw" / X
- https://x.com/BrivaelLp/status/1890435559127986463
- Brivael on X: "We are claiming SOTA for AI Avatar, but the ultimate test is big face. We don't use post process or blurring hacks to hide misery. 5 videos. Same script. Generate with Argil üëá https://t.co/DsFEhVCvBu" / X
- "Alibaba strikes again. Full-body swap anyone in a video with just a photo reference. What‚Äôs wild to me is that this tech completely bypasses the 3d pipeline (i.e. what Wonder Dynamics does to accomplish similar output) and yet looks so damn good. Basically viggle on steroids." / X https://x.com/bilawalsidhu/status/1890535455600369687
- https://pikartai.com/pikaddition/
- Pika AI: Pikadditions [Bring Your Video Imagination to Life]
- Got a few selfies? Turn them into a moving, talking Selfie Avatar in minutes. üì∏ Upload your photos üìù Enter a prompt (get creative!) üéôÔ∏è Record a short voiceover And boom - your Selfie Avatar video, delivered to your inbox. Give it a try for free: https://x.com/synthesiaIO/status/1889302506401849501
- ttps://yashkant.github.io/pippo/
- Pippo: High-Resolution Multi-View Humans from a Single Image h
- Meta presents: Pippo : High-Resolution Multi-View Humans from a Single Image Generates 1K resolution, multi-view, studio-quality images from a single photo in a one forward pass https://x.com/arankomatsuzaki/status/1889515688647373113
- https://humansensinglab.github.io/GAS/
- GAS: Generative Avatar Synthesis from a Single Image
- https://siyeoljung.github.io/DiffListener/
- DiffListener: Discrete Diffusion Model for Listener Generation
- EventEgo3D++: 3D Human Motion Capture from a Head Mounted Event Camera https://eventego3d.mpi-inf.mpg.de/
- https://agnjason.github.io/HumanDiT-page/
- HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation
- Rohan Paul on X: "This guy is on a Zoom call, but not actually in front of the camera. And you can also run it with text to speech open-source model trained on your own voice, written by a language model listening to the other user's voice input. https://t.co/BHyrgvVkVW" / X - https://x.com/rohanpaul_ai/status/1886757493998805226
- 2502.02465 - https://arxiv.org/pdf/2502.02465
- Towards Consistent and Controllable Image Synthesis for Face Editing
- 2501.18801 - https://arxiv.org/pdf/2501.18801
- Every Image Listens, Every Image Dances: Music-Driven Image Animation
- Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics - https://dress-1-to-3.github.io/
- https://kartik-3004.github.io/facexbench/
- FaceXBench
- Bilawal Sidhu on X: "‚ÄúBut AI will never get good at motion and physics‚Äù VideoJAM: hold my beer‚Ä¶ *Introduces motion priors, outperforms Runway, Sora &amp; Kling* Oh and works on any video model with minimal tweaks https://t.co/oSBT3f8t8p" / X - https://x.com/bilawalsidhu/status/1886874059503452234
- https://omnihuman-lab.github.io/
- TLDR: Video Diffusion Model using a Multi-Modal Diffusion Transformer (MMDiT), trained with flow matching and using a ton of data.
- ByteDance üé∂ has just released OmniHuman-1, and it's insanely good. It takes a single image and audio to produce these results.
- https://wyiguanw.github.io/WonderHuman/
- TLDR: Gaussian full body Avatars using Score Distillation Sampling in both Canonical and Posed Space.
- WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction


## 2024

### November 2024

- https://neuralbodies.github.io/RFGCA/
- Relightable Full-body Gaussian Codec Avatars
- https://x.com/pcuenq/status/1881978412270829728
- "Video Depth Anything is out! üöÄ Real-time inference for arbitrarily long videos with temporal and spatial consistency. Built on the excellent Depth Anything v2 (for images), by "simply" replacing the head and adjusting the loss for temporal consistency. Videos from the project
- nvlabs.github.io/FoundationStereo/ - https://nvlabs.github.io/FoundationStereo/
- FoundationStereo: Zero-Shot Stereo Matching
- https://dynamic-face.github.io/
- High-Quality and Consistent Video Face Swapping using Composable 3D Facial Priors
- Dynamic Face
- https://vrroom.github.io/synthlight/
- SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces
- https://lkjkjoiuiu.github.io/TalkingEyes_Home/
- TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation
- https://humanaigc.github.io/emote-portrait-alive-2/
- EMO2: End-Effector Guided Audio-Driven Avatar Video Generation
- https://arxiv.org/abs/2501.13452
- EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion
- https://www.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/
- Anyone want the script to run Moondream 2b's new gaze detection on any video? : r/LocalLLaMA
- https://x.com/rohanpaul_ai/status/1876213656645763098
- "Great Open-source model and paper from ByteDance Face dubbing made simple - from sound to lips in one smooth move. Skip the motion middleman, direct audio-to-lip sync in latent space eliminates the middleman for better expressions. LatentSync introduces end-to-end lip sync
- AI at Meta on X: "Released by @RealityLabs Research at #ECCV2024, Nymeria is a large-scale multimodal egocentric dataset for full-body motion understanding with potential applications in VR/MR headsets, smart glasses and more. More on this work + access to the dataset ‚û°Ô∏è https://t.co/V9vOBt46u8 https://t.co/w8eXEWcJeL" / X - https://x.com/AIatMeta/status/1877058463706325326
- https://x.com/reach_vb/status/1877100242534912068
- "HOLY SHIT - generate 3D mesh from a single image in LESS THAN A SECOND ü§Ø
- JANUARY 2025
- merve on X: "ViTPose -- best open-source pose estimation model just landed to @huggingface transformers üï∫üèªüíÉüèª See how to use on the next one ‚§µÔ∏è https://t.co/lQYvT064Wu" / X - https://x.com/mervenoyann/status/1877360767478952012
- https://x.com/Almorgand/status/1872598770179035191
- "DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding TL;DR: DINO-X Pro: sota model with enhanced perception capabilities for various scenarios; DINO-X Edge: model optimized for faster inference speed and better suited for deployment on edge devices
- https://kakituken.github.io/affordance-any.github.io/
- Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion
- https://x.com/XiaoxuanMa_/status/1870494395822395779
- "Excited to announce our new work SAT-HMR ü•≥ With a better balance of performance and efficiency, it's the best real-time 3D multi-person mesh estimation model. Check it out here:
- https://x.com/liuziwei7/status/1863965769979285894
- "üöÄTurn Single Image into 3D HumanüöÄ #GeneMAN is a generalizable single-image 3D human reconstruction framework that turns in-the-wild images into high-quality 3D humans with ease üîóProject:
- https://ppetrichor.github.io/levitor.github.io/
- LeviTor: 3D Trajectory Oriented <br> Image-to-Video Synthesis
- https://promptda.github.io/
- Prompt Depth Anything
- https://x.com/_akhaliq/status/1868535608370708643
- "Meta releases Apollo An Exploration of Video Understanding in Large Multimodal Models a family of state-of-the-art video-LMMs
- https://x.com/bilawalsidhu/status/1869472246131269878
- "Ed Catmull (Pixar founder) just joined Odyssey's board as they reveal their first 3D breakthrough: Instantly turning any 2D image into an editable 3D world. World Labs isn't alone anymore. Here's the TL;DR without the hype üßµ
- Viggle Adjacent‚Ä¶ but different methods‚Ä¶ converging.
- https://x.com/TheHumanoidHub/status/1869109219196375417
- "New paper presents Exbody2, a whole-body tracking framework enabling humanoid robots to mimic dynamic, human-like motions (e.g., running, dancing) with stability. Trained via RL in simulation and transferred to real robots, it outperforms prior methods.
- https://x.com/bbldCVer/status/1866020964271800830
- "Excited to share StableAnimator, an open-source human image animation model that excels in identity preservation! It transforms reference images into stunning, pose-driven animations with remarkable fidelity.
- https://arxiv.org/abs/2412.09296v1
- [2412.09296v1] GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with Rhythmic Poses and Realistic Expression
- https://x.com/bilawalsidhu/status/1868298708841902172
- "Check out this Stereo4D paper from Google DeepMind. It's a pretty clever approach to a persistent problem in computer vision -- getting good training data for how things move in 3D. The key insight is using VR180 videos -- those stereo fisheye videos we launched back in 2017 for
- https://x.com/maxescu/status/1865492523986063739
- "The new Runway Act-One update is wild I used the first 30 seconds from Johnny Harris's latest video to drive multiple AI-generated versions and changed his voice to a female one with ElevenLabs. 2025 is going to be out of this world üöÄ
- https://x.com/elevenlabsio/status/1866180148430774734
- "ElevenLabs x @hedra_labs Voice Your Character Competition. With our new Voice Design feature now in Hedra, you can create unique voices from a simple text prompt along with lip-synced character videos. To celebrate, we‚Äôre hosting a competition with $4k in prizes.
- https://x.com/markjeffrey/status/1867367669756047563
- "Ah! This was an experiment worth doing! I used kolorize @PhotoColorizer to normalize the color palette in the trailer for 'The Name and the Shadow'. Now, it all looks like a coherent whole, cut from the same movie, rather than a bunch of wildly different clips. Turn üîä up!
- https://x.com/matthieurouif/status/1866813819676205131
- "We built a AI model arena to rank the top background removal tools based on community votes. Photoroom wouldn't be here without open source so I am very proud of this opportunity to bring a transparent, open, reliable resource for everyone. On @huggingface. cc @julien_c
- https://huggingface.co/spaces/bgsys/background-removal-arena
- Background Removal Arena - a Hugging Face Space by bgsys
- https://x.com/bilawalsidhu/status/1867704947799986608
- "The future of sports broadcasting is volumetric. Combine this 3d tech with advances in object detection &amp; tracking, and you can tap a button to follow the ball or even a specific player. Split screen + stats + social for the full experience. Cant wait!
- https://x.com/janusch_patas/status/1867511260872683921
- "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos Contributions: 1) A framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video. 2) DynaDUSt3R, a method that takes a pair of frames
- https://x.com/vdeschaintre/status/1866142914885382152
- "üéì We introduce SAMa! A material selection and segmentation model on 3D models in any format (3DGS, NeRF, Mesh). Given a user click, we propose to select all regions on an objects with the same material. We can also do segmentation in under a minute:
- https://x.com/liyixxxuan/status/1866841261157781710
- "Excited to introduce Proc-GSüöÄ! A new pipeline integrating procedural modeling with 3D-GS to accelerate 3D building creation and infinite city generation with high flexibility and photo-realistic visual quality. -Project:
- https://x.com/ducha_aiki/status/1867556550661075243
- "LineGS : 3D Line Segment Representation on 3D Gaussian Splatting Chenggang Yang, Yuang Shi, Wei Tsang Ooi tl;dr: 3DGS tend to be dense along the edges -&gt; so they can help filtering &amp; post processing the lineclouds.
- https://x.com/Uncanny_Harry/status/1865379507923824799
- "The new Viggle 3.0 model is a big improvement, less of the viggle wiggle and more definition. We are getting loads of control in AI gen this weekend. Bravo team @ViggleAI
- https://x.com/zfkuang1/status/1862594009543450998
- "Recent video depth models thrive on large-scale annotated datasets‚Äîbut what if they‚Äôre unavailable? Introducing Buffer Anytime: a zero-shot framework using image priors to predict video depth and normals. Trained exclusively on unannotated videos, it achieves surprisingly smooth
- https://x.com/yuyinzhou_cs/status/1864500829040095737
- "üöÄ Introducing LayerDecomp: our latest generative framework for image layer decomposition, which can output photorealistic clean backgrounds and high-quality transparent foregrounds, faithfully preserving visual effects like shadows and reflections. Our key contributions include
- https://x.com/bilawalsidhu/status/1865067246038520157
- "Wav2lip can finally rest in peace. Being able to retarget the facial performance of characters in *existing* live action &amp; CG video makes Act-One an extremely useful tool for all types of creators. Nicely done Runway!
- https://x.com/YuanLiu41955461/status/1863149385170973135
- "Our new work PSHuman reconstructs a detailed 3D human mesh from a single-view image in ~1min. Codes have already been released! Welcome to try it! Project page:
- https://x.com/Hailuo_AI/status/1863961575574622662
- "Introducing Hailuo I2V-01-Live: Transform Static Art into Dynamic Masterpieces Live is our latest addition to the I2V family, designed to revolutionize how 2D illustrations come to life. With enhanced smoothness and vivid motion, this model lets your characters move, speak, and
- DECEMBER 2024
- SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory - https://yangchris11.github.io/samurai/
- https://x.com/bilawalsidhu/status/1860348056916369881
- "Just read a neat AI paper called SAMURAI -- it takes SAM 2 (Meta's "segment anything" model) and makes it way better at tracking objects in videos. Basic problem is SAM 2 gets confused when things move fast or there's a crowd of similar objects (big problem for VFX and video
- https://x.com/poolio/status/1861964631389950265
- "Stop watching videos, start interacting with worlds. Stoked to share CAT4D, our new method for turning videos into dynamic 3D scenes that you can move through in real-time!
- https://x.com/ChrisWu6080/status/1861964000646340630
- "üöÄ Introducing CAT4D! üöÄ CAT4D transforms any real or generated video into dynamic 3D scenes with a multi-view video diffusion model. The outputs are dynamic 3D models that we can freeze and look at from novel viewpoints, in real-time! Be sure to try our interactive viewer!
- https://x.com/ziqi__ma/status/1859634303296266536
- "How far are we from embodied intelligence? We first ask whether we can build a model to locate semantic concepts in 3D. Meet Find3Düîç: a model to locate any part of any 3D object based on any text query! üëâ
- https://x.com/bilawalsidhu/status/1862597954906870120
- "Monocular depth estimation has gotten so good lately. It used to look like a blurry blobby flickering mess. Now it looks like a synthetic depth map you‚Äôd export from a 3d tool Like putting on a fresh pair of glasses ü§ì
- https://antgroup.github.io/ai/echomimic_v2/
- EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation
- https://x.com/dreamingtulpa/status/1861366917317218327
- "3D Gaussian Splats are cool, but they're static (Part 37). GPS-Gaussian+ can render high-resolution 3D scenes from 2 or more input images in real-time! Links ‚¨áÔ∏è
- https://x.com/yunongliu1/status/1858951328980251065
- "üí´ü™ëIntroducing IKEA Manuals at Work: The first multimodal dataset with extensive 4D groundings of assembly in internet videos! We track furniture parts‚Äô 6-DoF poses and segmentation masks through the assembly process, revealing how parts connect in both 2D and 3D space. With
- https://x.com/gene_ch0u/status/1859419280468594972
- "We've released our paper "Generating 3D-Consistent Videos from Unposed Internet Photos"! Video models like Luma generate pretty videos, but sometimes struggle with 3D consistency. We can do better by scaling them with 3D-aware objectives. 1/N page:
- https://arxiv.org/abs/2411.04632v1
- [2411.04632v1] Improved Multi-Task Brain Tumour Segmentation with Synthetic Data Augmentation
- https://stdgen.github.io/
- StdGEN: Semantic-Decomposed 3D Character Generation from Single Images
- https://x.com/_akhaliq/status/1854222095477318096
- "AdvancedLivePortrait-WebUI Dedicated gradio based WebUI for ComfyUI-AdvancedLivePortrait edit the facial expression from the image
- https://x.com/_akhaliq/status/1854917732174573583
- "DimensionX Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion
- https://byteaigc.github.io/X-Portrait2/
- X-Portrait 2: Highly Expressive Portrait Animation
- https://x.com/MoveAI_/status/1848696832378966359
- "We‚Äôre excited to see creators like Ken @ODonnellKen push the boundaries of what‚Äôs possible using our markerless motion capture technology! Whether you‚Äôre an indie studio or a creative team, Move AI empowers you to bring your ideas to life at scale. üåç‚ú® #MoveAI
- https://abcyzj.github.io/MeshRet/
- Skinned Motion Retargeting with Dense Geometric Interaction Perception
- MeshRet
- https://noposplat.github.io/
- Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images
- NoPoSplat
- https://haian-jin.github.io/projects/LVSM/
- LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias
- https://cvlab-kaist.github.io/PF3plat/
- PF3plat : Pose-Free Feed-Forward 3D Gaussian Splatting
- https://ykdai.github.io/projects/InclusionMatching
- Learning Inclusion Matching for Animation Paint Bucket Colorization
- https://huggingface.co/spaces/fffiloni/EchoMimic
- Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning
- EchoMimic - a Hugging Face Space by fffiloni
- https://x.com/MartinNebelong/status/1843409578438107519
- "Look at the realism here! Hot take: This blows most 'old-school' CG humans out of the water. üòÆ "But you have no control".. it's getting there! Using 'Facepoke' as input for Runway's video-to-video, it's clear we're entering a new era of creation. #AIArt #GenerativeAI

### October 2024

- (4) enigmatic_e on X: "Now the BTS. I recorded myself lip-syncing the song to transfer my facial movements onto images and videos. The parts where there‚Äôs other movements, I used Runway and Lumalabs to get the images to move and used liveportrait after to get the mouth movements. #kanye #ye #comfyui https://t.co/4nFAA9TsSc" / X - https://twitter.com/8bit_e/status/1839732374336090625
- (4) Halim Alrasihi on X: "This is Hollywood grade lip-syncing: Here is how to create an AI avatar with accurate and realistic Lip-Sync. This might be currently the best tool for video-to-video right now. Only 4 steps are needed: https://t.co/TGYuRLMW0g" / X - https://twitter.com/HalimAlrasihi/status/1839310216602788103
- https://twitter.com/8bit_e/status/1844435081869525489
- I had to test how Gen-3‚Äôs video-to-video would perform using a Viggle output. Have any of you done any interesting with vid2vid? https://t.co/7pxTJKwGag‚Äù
- https://cotracker3.github.io/
- CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos
- https://huggingface.co/spaces/facebook/cotracker
- üé® CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos
- https://pantomatrix.github.io/TANGO/
- TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio-Motion Embedding and Diffusion Interpolation

### September 2024

- (4) enigmatic_e on X: "A little experiment I worked on using LivePortrait, Viggle, and ComfyUI. Will be doing a breakdown soon! https://t.co/sTWSjGXKaz" / X - https://twitter.com/8bit_e/status/1836845874468839612?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1836845874468839612%7Ctwgr%5Ec2ca01bc19492dfac671858b9f413a4357e47a78%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F09%2F21%2Fai-news-51-week-ending-09-20-2024-with-executive-summary-top-80-links-and-helpful-visuals%2F
- https://x.com/8bit_e/status/1833250941006491876
- "Finally got around to really dig deeper into LivePortrait.

### July 2024

- https://twitter.com/_vztu/status/1819838124161200223
- "üö®Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360¬∞ üåüùêèùê´ùê®ùê£:
- Model a full head from a single portrait image
- https://twitter.com/_akhaliq/status/1811583979067207960
- "Generalizable Implicit Motion Modeling for Video Frame Interpolation Motion modeling is critical in flow-based Video Frame Interpolation (VFI). Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps
- https://twitter.com/_akhaliq/status/1814156712175083539
- "Shape of Motion 4D Reconstruction from a Single Video Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective
- AUGUST 2024
- https://twitter.com/fdaudens/status/1811029049638011000
- "Cool demo by @mervenoyann for real-time object tracking with RT-DETR
- https://news.mit.edu/2024/researchers-leverage-shadows-model-3d-scenes-blocked-objects-0618
- Researchers leverage shadows to model 3D scenes, including objects blocked from view | MIT News | Massachusetts Institute of Technology
- https://haoosz.github.io/ConceptExpress/
- ConceptExpress: Unsupervised Concept Extraction (UCE): We focus on the unsupervised problem of extracting multiple concepts from a single image. Given an image that contains multiple concepts, we aim to harness a frozen pretrained diffusion model to automatically learn the conceptual tokens. Using the learned conceptual tokens, we can regenerate the extracted concepts with high quality.
- Efficient Portrait Animation with Stitching and Retargeting Control - https://liveportrait.github.io/
- 2407.03168 - https://arxiv.org/pdf/2407.03168
- Live Portrait - a Hugging Face Space by KwaiVGI - https://huggingface.co/spaces/KwaiVGI/LivePortrait
- MCNet: Talking Head Video Generation - https://goatstack.ai/topics/mcnet-talking-head-video-generation-gsgchf
- [2307.09906] Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation - https://arxiv.org/abs/2307.09906
- This Free AI Video Tool Brings Characters to Life - YouTube - https://www.youtube.com/watch?v=cucaEEDYmsw&t=2s
- [2407.05712] MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices - https://arxiv.org/abs/2407.05712
- MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices | AI Research Paper Details - https://www.aimodels.fyi/papers/arxiv/mobileportrait-real-time-one-shot-neural-head

### May 2024

- Aran Komatsuzaki on X: "TikTok presents Depth Anything V2 Trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation model proj: https://t.co/KaOQauiOST abs: https://t.co/9HxIpsPWJJ https://t.co/aj9S1SKjzN" / X - https://twitter.com/arankomatsuzaki/status/1801435793254121798?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1801435793254121798%7Ctwgr%5E3b1555acae76e0fd9879fd4ccb60a782b4dc2a93%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F14%2Fmultimodality-news-week-ending-06-14-2024%2F
- AK on X: "Depth Anything V2 This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more https://t.co/s7TNDJIDvQ" / X - https://twitter.com/_akhaliq/status/1801432403665125738?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1801432403665125738%7Ctwgr%5E3b1555acae76e0fd9879fd4ccb60a782b4dc2a93%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F14%2Fmultimodality-news-week-ending-06-14-2024%2F
- Florent Daudens on X: "New vision model from @Microsoft, Florence-2 - Can perform various tasks: object detection, grounding, segmentation, OCR - 200M and 800M models https://t.co/Wp8LMjkM0M" / X - https://twitter.com/fdaudens/status/1803427363746717866?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1803427363746717866%7Ctwgr%5E1e807c2a883c66c70805660a846157e2d1ee1440%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F21%2Fmicrosoft-ai-news-week-ending-06-21-2024%2F
- JUNE 2024
- Jim Fan on X: "We've found a cozy home for our robots in the world of bits! RoboCasa: a place where robot arms, dogs, and humanoids can train safely for daily tasks in procedurally generated simulations. It's all open-source, check it out in Yuke's thread!" / X - https://twitter.com/DrJimFan/status/1795482053053669687?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1795482053053669687%7Ctwgr%5E4a203dcd9558e4536c5e7700b6612e6c7df68c5f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F02%2Frobotics-and-embodiment-news-week-ending-05-31-2024%2F
- Reason3D - https://kuanchihhuang.github.io/project/reason3d/
- Amar Singh on X: "In honor of the playoffs, I‚Äôd like to showcase what we‚Äôve been working on here at Nexavision ‚Äî a new way to generate basketball analytics through tracking with computer vision and AI: üßµ https://t.co/DgC3BBXr38" / X - https://twitter.com/AmarSVS/status/1793037268690579787
- Jim Fan on X: "What makes up the abstract concept of an apple? We read the word "apple" as a string, see 2D pictures online, 3D shape in real life, and moving apples in videos. We touch the apple, feel its geometry in our palms and texture through the rich tactile sensation on our fingers. Do https://t.co/2LzxYa4f3N" / X - https://twitter.com/DrJimFan/status/1793318771932995793?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1793318771932995793%7Ctwgr%5Ed9dcfb501f151309c4a242c5a48759e1425e6ad1%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F05%2F24%2Fmultimodality-news-week-ending-05-24-2024%2F
- Francis Engelmann on X: "I'll be at @iclr_conf in Vienna next week, presenting OpenNeRF (Open-Set 3D Neural Scene Segmentation) üìùPaper https://t.co/9MUl3PnhbA üë©‚ÄçüíªCode https://t.co/vLZMlx0iZr üöÄProject https://t.co/1R8qnW3QQv w/@fedassa @mapo1 @Mi_Niemeyer et al. Zero-shot unsupervised 3D segmentation: https://t.co/iWSAOzvU5r" / X - https://twitter.com/FrancisEngelman/status/1787133195973984749?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1787133195973984749%7Ctwgr%5E3a0929c0188ecefc08d3f027e68d0c98cfaa43c0%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F05%2F10%2Fopen-source-ai-news-week-ending-05-10-2024%2F
- Amar Singh on X: "In honor of the playoffs, I‚Äôd like to showcase what we‚Äôve been working on here at Nexavision ‚Äî a new way to generate basketball analytics through tracking with computer vision and AI: üßµ https://t.co/DgC3BBXr38" / X - https://twitter.com/AmarSVS/status/1793037268690579787
- Moving Object Segmentation: All You Need Is SAM (and Flow) - https://www.robots.ox.ac.uk/~vgg/research/flowsam/

### April 2024

- Jim Fan on X: "one day PhDs will animate every object around us with reinforcement learning to keep their thesis going https://t.co/A9ZNNKlmwP" / X - https://twitter.com/DrJimFan/status/1778819699062735350
- The latest HeyGen avatars - quick demo - Ethan B. Holland - https://ethanbholland.com/2024/04/26/the-latest-heygen-avatars-quick-demo/
- A Visual Guide to Vision Transformers | MDTURP - https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html
- HeyGen on X: "[ùó°ùóòùó™]‚ú®ü§≥üèΩ URL ‚û°Ô∏è UGC Meet Gail, Tim, Lukas, Daran, Lea, and Kim ‚Äî HeyGen's newest UGC-style avatars! 1Ô∏è‚É£ Input a URL (from Amazon, a Shopify store, etc.) 2Ô∏è‚É£ Customize title, description, media, script, and avatar 3Ô∏è‚É£ Create content in seconds for social media, emails, etc. https://t.co/DQCCM7f5VX" / X - https://twitter.com/HeyGen_Official/status/1780597444335575108
- The Viggle AI Meme's Impact on Image-to-Video Awareness - Ethan B. Holland - https://ethanbholland.com/2024/04/08/the-viggle-ai-memes-impact-on-image-to-video-awareness/
- Dreaming Tulpa ü•ìüëë on X: "SAM + Optical Flow = FlowSAM FlowSAM can discover and segment moving objects in a video and outperforms all previous approaches by a considerable margin in both single and multi-object benchmarks üî• https://t.co/qjaMQOYtlN https://t.co/mp2MTlehV6" / X - https://twitter.com/dreamingtulpa/status/1782083341883224462

### March 2024

- Figure Status Update - OpenAI Speech-to-Speech Reasoning - YouTube - https://www.youtube.com/watch?v=Sq1QZB5baNw
- benjamin-paine/aniportrait ¬∑ Hugging Face - https://huggingface.co/benjamin-paine/aniportrait
- https://arxiv.org/abs/2403.17694
- AniPortrait
- Image Animation Using Thin Plate Spline Motion Model - a Hugging Face Space by CVPR - https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model
- HeyGen on X: "[NEW] ‚Äî Avatar in Motion 1.0üèÉ‚Äç‚ôÇÔ∏è‚ú® Move your hands, make gestures, use unique tones of voice, and HeyGen will flawlessly track, translate, and lip-sync your video with any input text. Can't believe it? üëáComment a script for what we should make Nik say &amp; we‚Äôll post the video! https://t.co/d1X75Z5m2S" / X - https://twitter.com/HeyGen_Official/status/1773119891068883240
- ViggleAI on X: "VIGGLE'S NEW MODEL IS HERE! Controllable video generation is our mission, and character is where we start. Animate any character just as you want, simply with a prompt, or by uploading a reference video featuring clear motion. Try Viggle for FREE: https://t.co/11cDhfqisU! https://t.co/hVWWyE4IOa" / X - https://twitter.com/ViggleAI/status/1768224856406131110?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1768224856406131110%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F


## 2023

### August 2023

- NVIDIA NIM | audio2face - https://build.nvidia.com/nvidia/audio2face
- NVIDIA Omniverse Audio2Face App - YouTube - https://www.youtube.com/playlist?list=PL3jK4xNnlCVfIJU10iQ_dd8J_5T_e0YTJ

### July 2023

- Dreaming Tulpa ü•ìüëë on X: "TikTok presents Boximator! This method can generate rich and controllable motions for image-to-video generations by drawing box constraints and motion paths onto an image and combining it with a prompt: "A girl in red is covering her face with a skull." 10 crazy examples: https://t.co/Jy3op9BEdB" / X - https://x.com/dreamingtulpa/status/1756965825657925967
- Hieu üöÄ on X: "This AI helps you go to sleep real fast. Kidding, it counts sheep https://t.co/sYW1R7NWi4" / X - https://twitter.com/hieuSSR/status/1754808953480032546?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1754808953480032546%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- SkalskiP on X: "REAL-TIME object detection WITHOUT TRAINING YOLO-World is a new SOTA open-vocabulary object detector that outperforms previous models in terms of both accuracy and speed. 35.4 AP with 52.0 FPS on V100. ‚Üì read more https://t.co/SoqFyEk41V" / X - https://twitter.com/skalskip92/status/1754916529672438173
- Bilawal Sidhu on X: "AI can count sheep. But it never needs to go to sleep. It can see everything, everywhere, all at once‚Ä¶ at a certain P/R for certain semantic classes ;) üêë ‚òÅÔ∏è üêë ‚òÅÔ∏è https://t.co/6f1cXSLEOG" / X - https://twitter.com/bilawalsidhu/status/1755071169047277939
- Micah Berkley on X: "I'm very impressed with new Alibaba's Vision AI model. In some way's it's better than GPT4 because it does an extremely impressive job at identifying people. The way that I use these vision models in business is usually to interface images into words, or in my case, I use them https://t.co/mgUvBLQXRe" / X - https://twitter.com/MicahBerkley/status/1754019702320374070
- #CVPR2024 on X: "CVPR 2007 Best Paper Award Dynamic 3D Scene Analysis from a Moving Vehicles B. Leibe, N. Cornelis, K. Cornelis, L. Van Gool #TBThursday https://t.co/weeChSRYJk" / X - https://twitter.com/CVPR/status/1758129034008154401
- Gorden Sun on X: "Depth AnythingÔºöÊääÂõæÁâáÂíåËßÜÈ¢ëËΩ¨ÂåñÊàêÊ∑±Â∫¶Âõæ Â≠óËäÇÊé®Âá∫ÁöÑÊ®°ÂûãÔºåÊïàÊûúÈùûÂ∏∏Â•ΩÔºåÂÆûÈôÖÊïàÊûúËßÅËßÜÈ¢ë„ÄÇ È°πÁõÆÂú∞ÂùÄÔºöhttps://t.co/QHbp8tPQbC Â§ÑÁêÜËßÜÈ¢ëÁöÑÂú®Á∫ø‰ΩìÈ™åÔºöhttps://t.co/ROdrmoIF2G Â§ÑÁêÜÂõæÁâáÁöÑÂú®Á∫ø‰ΩìÈ™åÔºöhttps://t.co/hnuxjjQoWM GithubÔºöhttps://t.co/76X2Cv7Tb8 https://t.co/mGc3l1wFbs" / X - https://twitter.com/Gorden_Sun/status/1757046907917275608?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1757046907917275608%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Feb 2024
- Dreaming Tulpa ü•ìüëë on X: "TikTok presents Boximator! This method can generate rich and controllable motions for image-to-video generations by drawing box constraints and motion paths onto an image and combining it with a prompt: "A girl in red is covering her face with a skull." 10 crazy examples: https://t.co/Jy3op9BEdB" / X - https://twitter.com/dreamingtulpa/status/1756965825657925967
- #CVPR2024 on X: "CVPR 2007 Best Paper Award Dynamic 3D Scene Analysis from a Moving Vehicles B. Leibe, N. Cornelis, K. Cornelis, L. Van Gool #TBThursday https://t.co/weeChSRYJk" / X - https://twitter.com/CVPR/status/1758129034008154401
- Gorden Sun on X: "Depth AnythingÔºöÊääÂõæÁâáÂíåËßÜÈ¢ëËΩ¨ÂåñÊàêÊ∑±Â∫¶Âõæ Â≠óËäÇÊé®Âá∫ÁöÑÊ®°ÂûãÔºåÊïàÊûúÈùûÂ∏∏Â•ΩÔºåÂÆûÈôÖÊïàÊûúËßÅËßÜÈ¢ë„ÄÇ È°πÁõÆÂú∞ÂùÄÔºöhttps://t.co/QHbp8tPQbC Â§ÑÁêÜËßÜÈ¢ëÁöÑÂú®Á∫ø‰ΩìÈ™åÔºöhttps://t.co/ROdrmoIF2G Â§ÑÁêÜÂõæÁâáÁöÑÂú®Á∫ø‰ΩìÈ™åÔºöhttps://t.co/hnuxjjQoWM GithubÔºöhttps://t.co/76X2Cv7Tb8 https://t.co/mGc3l1wFbs" / X - https://twitter.com/Gorden_Sun/status/1757046907917275608
- Emad on X: "Pretty much, just describe to edit. In the sora technical paper they note that they apply SDEdit by @chenlin_meng et al fine - you can take that further through scene decomposition, segmentation, regional prompting &amp; more to autocorrect &amp; balance image, video, audio, etc etc https://t.co/C2k4vf2bkm" / X - https://twitter.com/EMostaque/status/1758854642635526625
- SkalskiP on X: "REAL-TIME object detection WITHOUT TRAINING YOLO-World is a new SOTA open-vocabulary object detector that outperforms previous models in terms of both accuracy and speed. 35.4 AP with 52.0 FPS on V100. ‚Üì read more https://t.co/SoqFyEk41V" / X - https://twitter.com/skalskip92/status/1754916529672438173?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1754916529672438173%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Feb 2024
- Bo Wang on X: "üéâ The best way to start the week is to find out that our MedSAM is finally published today in Nature Communications! **Segment anything in medical images** Paper: https://t.co/CFKykvt07n arXiv: https://t.co/QOC9KaCg41 Data &amp; Code: https://t.co/r05Agl59xY MedSAM is the first https://t.co/2tuEOgtel8" / X - https://twitter.com/BoWang87/status/1749623699546788124
- pix2gestalt: Amodal Segmentation by Synthesizing Wholes - https://gestalt.cs.columbia.edu/
- HeyGen on X: "Introducing NEW AI Avatars! üßë‚Äçü§ù‚Äçüßë üìçNew and Diverse Avatars üìçTeam Branding Kits enable you to maintain a consistent feel, ensuring a unified brand voice and appearance to keep your business messaging aligned. üìçNew Generative Outfits https://t.co/DJcX9xUJ08" / X - https://twitter.com/HeyGen_Official/status/1747697831207485541?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1747697831207485541%7Ctwgr%5E725844436ea18e7baf1e3b7157641de755fe0415%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F01%2F20%2Fai-video-news-week-ending-01-19-2024%2F
- sync.labs (YC W24) on X: "‚ö° launch + monetize your own AI dubbing app in &lt;30 mins drop a YouTube link + translate videos w/ perfect lip-sync curious? we open sourced the whole repo üöÄ try it out / clone it here üëá https://t.co/39tWvYErqD" / X - https://twitter.com/synclabs_so/status/1752496443003371637?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1752496443003371637%7Ctwgr%5Ee9ade09f48f864c11e0a15bd405f1f88df483a97%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F02%2F07%2Fai-video-news-week-ending-02-02-2024%2F
- OMG-Seg - https://lxtgh.github.io/project/omg_seg/
- Jim Fan: The next grand challenge for AI | TED Talk - https://www.ted.com/talks/jim_fan_the_next_grand_challenge_for_ai?subtitle=en
- ByteDance Introduces MagicVideo-V2: A Groundbreaking End-to-End Pipeline for High-Fidelity Video Generation from Textual Descriptions - MarkTechPost - https://www.marktechpost.com/2024/01/16/bytedance-introduces-magicvideo-v2-a-groundbreaking-end-to-end-pipeline-for-high-fidelity-video-generation-from-textual-descriptions/
- Jan 2024
- AK on X: "Bytedance announces DiffPortrait3D Controllable Diffusion for Zero-Shot Portrait View Synthesis paper page: https://t.co/lOpCdNXgUi given an unposed portrait image, diffportrait3d can synthesize plausible but consistent facial details with retained both identity and facial https://t.co/AODei632q9" / X - https://twitter.com/_akhaliq/status/1738247956019524048?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1738247956019524048%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Lukas Hoyer on X: "üöÄIntroducing SemiVL - a semi-supervised semantic segmentation method exploiting priors from vision-language models üìÑPaper: https://t.co/bEOBn17vD5 üíªCode: https://t.co/4qnEHv2uZb üòäTeam: @lukashoyer3, David J. Tan, @ferjadnaeem, Luc Van Gool, @fedassa #ETHZurich #Google https://t.co/4Ix4RTu60A" / X - https://twitter.com/lukashoyer3/status/1737837511433666699?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737837511433666699%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- merve on X: "Think of an LLM that can find entities in a given image, describe the image and answers questions about it, without hallucinating ‚ú® Kosmos-2 released by @Microsoft is a very underrated model that can do that. Code snippet with transformers integration is in the next tweet üëá https://t.co/tscJEygdz7" / X - https://twitter.com/mervenoyann/status/1737506720249782495?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737506720249782495%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- Aran Komatsuzaki on X: "Tracking Any Object Amodally Infers complete object boundaries, even when certain portions are occluded. proj: https://t.co/TaruN0WQ61 abs: https://t.co/JqsIRFQkFK https://t.co/4crKpIgfZv" / X - https://twitter.com/arankomatsuzaki/status/1737309414443438221?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737309414443438221%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- Magic Animates - Bring Your Photos to Dance - https://magicanimates.com/
- Dec 2023
- Segmentation-Based Parametric Painting - https://manuelladron.github.io/semantic_based_painting/
- 2311.02549 - https://arxiv.org/pdf/2311.02549
- Latent consistency
- AI Art Just Changed Forever - YouTube - https://www.youtube.com/watch?v=4-2dSRjErE4&t=4s
- Nov 2023
- Unsupervised Object Segmentation - https://vlar-group.github.io/UnsupObjSeg.html
- Zhe Gan on X: "üöÄüöÄIntroducing Ferret, a new MLLM that can refer and ground anything anywhere at any granularity. üì∞https://t.co/gED9Vu0I4y 1‚É£ Ferret enables referring of an image region at any shape 2‚É£ It often shows better precise understanding of small image regions than GPT-4V (sec 5.6) https://t.co/yVzgVYJmHc" / X - https://twitter.com/zhegan4/status/1712345137983201716?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1712345137983201716%7Ctwgr%5Ed10d666cd7f2c97432d9d29aa4c1796f6c84c175%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F01%2F07%2Fai-news-12-week-ending-12-29-2023-with-executive-summary-and-top-9-stories%2F
- Nat Friedman on X: "But there's more: a few days ago, @Youssef_M_Nader, working with new segmentations from the Vesuvius Challenge segmentation team, was able to generate this image of stunning clarity and size: This is the revolution in digital papyrology that Dr. Seales has created. Nothing like https://t.co/oFyzYpsjRi" / X - https://twitter.com/natfriedman/status/1712475202008109526?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1712475202008109526%7Ctwgr%5E6d9331d3195dc5c357a8844c99e18e74f44f03a7%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F10%2F13%2Fai-news-2-week-ending-10-13-2023%2F
- October
- AK on X: "Tracking Anything with Decoupled Video Segmentation paper page: https://t.co/xqfwTkf78V Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To https://t.co/6FSShhURHV" / X - https://twitter.com/_akhaliq/status/1700030823926280448?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700030823926280448%7Ctwgr%5Eec61b21c307f753813fc43c4b40832eddd43e735%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F10%2F06%2Fai-news-week-ending-10-6-2023%2F
- AI Mid-Term Outlook: Ethan Holland Presentation at the National Association of Broadcasters - Sept 20, 2023 - Ethan B. Holland - https://ethanbholland.com/2023/09/20/hello-world/
- AK on X: "Tracking Anything with Decoupled Video Segmentation paper page: https://t.co/xqfwTkf78V Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To https://t.co/6FSShhURHV" / X - https://twitter.com/_akhaliq/status/1700030823926280448?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700030823926280448%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Sept 2023
- Efficient Video Portrait Reenactment via Grid-based Codebook | ACM SIGGRAPH 2023 Conference Proceedings - https://dl.acm.org/doi/abs/10.1145/3588432.3591509

### April 2023

- https://www.catalyzex.com/paper/face-animation-with-an-attribute-guided/code
- [2304.03199] Face Animation with an Attribute-Guided Diffusion Model - https://arxiv.org/abs/2304.03199


## 2022

### March 2022

- DaGAN: Depth-Aware Generative Adversarial Network for Talking Head Video Generation - https://harlanhong.github.io/publications/dagan.html
- DaGAN - a Hugging Face Space by HarlanHong - https://huggingface.co/spaces/HarlanHong/DaGAN
- [2203.06605] Depth-Aware Generative Adversarial Network for Talking Head Video Generation - https://arxiv.org/abs/2203.06605


## 2021

### April 2021

- GitHub - happy-jihye/face-vid2vid-demo - https://github.com/happy-jihye/face-vid2vid-demo
- tcwang0509/TalkingHead-1KH - https://github.com/tcwang0509/TalkingHead-1KH?tab=readme-ov-file
- face-vid2vid - https://nvlabs.github.io/face-vid2vid/


## 2020

### January 2020

- First Order Motion Model for Image Animation - YouTube - https://www.youtube.com/watch?v=u-0cQ-grXBQ


## 2019

### December 2019

- First order model - https://aliaksandrsiarohin.github.io/first-order-model-website/
