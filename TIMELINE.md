## 2025

### September 2025
- [2508.19242] Autoregressive Universal Video Segmentation Model
https://arxiv.org/abs/2508.19242
- USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning
https://bytedance.github.io/USO/ 
- ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling"" TL;DR: high-fidelity 3D humans across a wide range of poses, capturing both skeletal structure and surface details; separates internal skeleton from the external surface, (1/3)  https://x.com/Almorgand/status/1962581481055797586
- Unified Style and Subject-Driven Generation via Disentangled and Reward Learning
https://huggingface.co/spaces/bytedance-research/USO 
- HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting https://yimin-pan.github.io/hair-gs/ 
- Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation
Feed-forward 3D and 4D scene generation from a single image/video trained with synthetic data generated by a camera-controlled video diffusion model
https://x.com/_akhaliq/status/1970949464606245139
- "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views"
https://x.com/Almorgand/status/1970910944948781195

### August 2025
- TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis
https://freedomintelligence.github.io/talk-vid/ 
- CONVERGENCE WITH ROBOTS: Masquerade: Learning from In-the-wild Human Videos using Data-Editing (so many of these simulation videos look like human avatar animations)
https://masquerade-robot.github.io/ 
- PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image
https://mks0601.github.io/PERSONA/
- Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization
https://huggingface.co/papers/2508.14811 
- Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning
https://www.pair.toronto.edu/Adapt3R/
- FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers https://fantasy-amap.github.io/fantasy-portrait/
- Relightable Full-body Gaussian Codec Avatars https://neuralbodies.github.io/RFGCA/
- RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer https://markson14.github.io/RAP/
- HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis
https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/ 
- Alexandre Morgand on X: ""Physically Controllable Relighting of Photographs" TLDR: Explicit control of light sources akin to CG pipelines; different types of light sources; self-supervised training methodology using differentiable rendering to train their neural renderer with real-world photograph.â€
https://x.com/Almorgand/status/1952757175144558966 
- Alexandre Morgand on X: ""Cameras as Relative Positional Encoding" TLDR: comparison for conditioning transformers on cameras: token-level raymap, attention-level relative pose encodings, a (new) relative encoding Projective Positional Encoding -&gt; camera frustums, (int|ext)insics for relative pos encoding https://t.co/4ARQ3Ok6ID" / X
https://x.com/Almorgand/status/1951331762463822212
- No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views
https://ranrhuang.github.io/spfsplat/ 
- Meta Ã— Niantic is a big deal. Quest gets Lightship-grade 3D mapping, spatial anchors, and VPS localization - plus primitives like object detection &amp; semantic segmentation. Thatâ€™s the spatial computing stack shipped into the hands of devs today.  https://x.com/bilawalsidhu/status/1953822369622925607
- LightSwitch: Multi-view Relighting with Material-guided Diffusion TL;DR: material-relighting diffusion framework; relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties; (1/2)  https://x.com/Almorgand/status/1955655723985309967
- TANGENT: Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control
https://diffusecloc.github.io/website/ 

### July 2025
- Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars
https://tobias-kirschstein.github.io/avat3r/
- ICCV Poster ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling
https://iccv.thecvf.com/virtual/2025/poster/2270
- [2502.13133] AV-Flow: Transforming Text to Audio-Visual Human-like Interactions
https://arxiv.org/abs/2502.13133
- DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework
https://virtu-lab.github.io/ 
- CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering
https://clift-nvs.github.io/
- Data-efficient and Accurate Vision Models from Synthetic Data
https://microsoft.github.io/DAViD/ 
- Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars https://research.nvidia.com/labs/dair/dream-lift-animate/
- Tangentially related: How soon until AI can continuously fuse together all sensor data into a persistent 4D model of reality? https://x.com/bilawalsidhu/status/1947474834973131158
- Tangentially related: Huge. Take any image (real or synthetic) and turn it into a multi-part 3D object using @Scenario_gg.  https://x.com/bilawalsidhu/status/1947673321014735099 
- VoluMe: Authentic 3D Video Calls from Live Gaussian Splat Prediction
https://microsoft.github.io/VoluMe/ 
- Tangential: Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos"" TL;DR : Video diffusion model for dynamic novel-view synthesis trained in a self-supervised manner using only 2D videos. Novel-view synthesis as a structured inpainting task: (2/3)  https://x.com/Almorgand/status/1947265806288515448 
- Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors
https://dubbingforeveryone.github.io/ 
- HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars
https://bjkim95.github.io/haircup/ 
- MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second
https://chenguolin.github.io/projects/MoVieS/
- Mirage
https://mirage.decart.ai/queue?gameId=camera
- SoulDance: Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling
https://xjli360.github.io/SoulDance/
- StreamME: Simplify 3D Gaussian Avatar within Live Stream
https://songluchuan.github.io/StreamME/ 
- Runway on X: "Introducing Act-Two, our next-generation motion capture model with major improvements in generation quality and support for head, face, body and hand tracking. Act-Two only requires a driving performance video and reference character. 
https://x.com/runwayml/status/1945189222542880909 
- Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction
https://jiahao-ma.github.io/puzzles/ 
- SnapMoGen: Human Motion Generation from Expressive Texts
https://snap-research.github.io/SnapMoGen/ 
- Try on looks and discover your style with Doppl
https://blog.google/technology/google-labs/doppl/
- Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation https://digital-salon.github.io/
- GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering TL;DR: Video stabilization task with feed-forward 3DGS reconstruction, ensuring robustness to diverse motions, full-frame rendering and high geometry consistency. https://x.com/Almorgand/status/1940449877001183717
- ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions https://arxiv.org/pdf/2507.10542
- Introducing BlenderFusion: Reassemble your visual elementsâ€”objects, camera, and backgroundâ€”to compose a new visual narrative. Play the interactive demo: https://t.co/zJEZ4uhrwe 
https://x.com/sanghyunwoo1219/status/1939730467206619560
- VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis https://arxiv.org/pdf/2507.06060
- FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution
https://eyeline-research.github.io/FlashDepth/ 
- Meta AI Seamless Interaction with Avatars
Advancing AI research modeling of face-to-face dynamics, including expressive gestures, active listening, turn-taking and visual synchrony.
https://ai.meta.com/research/seamless-interaction/ 
- Modeling natural conversational dynamics with Seamless Interaction
https://ai.meta.com/blog/seamless-interaction-natural-conversational-dynamics/ 
- SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation
https://lucaria-academy.github.io/SynMotion/
- X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents
https://byteaigc.github.io/X-Unimotion/ 


### June 2025
- Veo 3 for Developers - Paige Bailey - YouTube
https://www.youtube.com/watch?v=hlcAZ2lX_ZI 
- 3DGH: 3D Head Generation with Composable Hair and Face 
https://c-he.github.io/projects/3dgh/ 
- Try on looks and discover your style with Doppl
https://blog.google/technology/google-labs/doppl/ 
- Higgsfield Soul
Higgsfield's first high-aesthetic photo model
https://higgsfield.ai/soul 
- AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models
https://anima-x.github.io/ 
- [2506.15442] Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material
https://arxiv.org/abs/2506.15442 
- UFM: A Simple Path towards Unified Dense Correspondence with Flow
https://uniflowmatch.github.io/ 
- VideoRefer VideoLLaMA3 - a Hugging Face Space by lixin4ever
Other models can answer â€œwhatâ€™s happening in the videoâ€ 
This one can answer questions on objects, you can prompt point prompts to any object in a timestamp (like SAM2!) 
It includes a separate object encoder on top of its image encoder
https://huggingface.co/spaces/lixin4ever/VideoRefer-VideoLLaMA3 
- Controllable and Expressive One-Shot Video Head Swapping
https://humanaigc.github.io/SwapAnyHead/ 
- HeyGen just dropped Voice Mirroring, it can clone anyone's voice emotion, tone and style PERFECTLY Â· <https://x.com/EHuanglu/status/1919696421625987220>
- HeyGen AI Studio Â· <https://www.heygen.com/blog/introducing-ai-studio>
- Introducing HunyuanVideo-Avatar, a model jointly developed by Tencent Hunyuan and Tencent Music, bringing photos to life. Â· <https://x.com/TencentHunyuan/status/1927575170710974560>
- Introducing Mirage Studio. Powered by our proprietary omni-modal foundation model. Generate expressive videos at scale, with actors that actually look and feel alive. Our actors laugh, flinch, sing, rap â€” all of course, per your direction. Â· <https://x.com/getcaptionsapp/status/1929554635544461727>
- These wild street interviews have completely taken over my feed. AI video had hit a quality bar already, but Veo 3 with native audio output has unlocked whole new category of creators (and thus content). Ppl who would've never bothered duct taping multiple tools together can  https://x.com/bilawalsidhu/status/1929568408820949350
- Monocular pose estimation has gotten really good Grab any 2D video and transfer the performance to a 3D character https://x.com/bilawalsidhu/status/1928612111896174870
- Tangentially related â€œWorld Labs open sourced their 3d gaussian splat renderer; built as a first-class ThreeJS citizen instead of bolting it on. Multiple splats, proper scene hierarchy, can transform and animate them, mix with regular meshes, plus works on mobile and VR.â€ Â· <https://x.com/bilawalsidhu/status/1929670896416895125>
- Introducing Mirage Studio
- Discover Mirage Studio â€” the fastest way to generate studio-quality videos with lifelike AI actors. Create high-performing content at scale without cameras, crews, or production delays. Â· <https://www.captions.ai/blog-post/introducing-mirage-studio>
- HERE WE GO
- John Carmack on X: "The beta 3D photo integration with Instagram is very well done! Every static photo becomes an AI generated stereoscopic 3D photo, and there is a â€œ3Dâ€ button that lets you toggle the feature on and off for comparison. Every photo I looked at â€œjust workedâ€, with no glaring" / X Â· <https://x.com/ID_AA_Carmack/status/1933199948759146810>
- Social Goes Spatial & the Beginning of Our Reimagined OS | Meta Quest Blog | Meta Store Â· <https://www.meta.com/blog/instagram-3d-photos-spatial-medianavigator-meta-quest-test/>
- Voice cloning is now trivially easy with open source tools, while live avatar videos of real people are easy with proprietary tools &amp; a variety of open source tools are getting there. Very limited time to adjust legal &amp; financial safeguards to new ways of authenticating people"" / X https://x.com/emollick/status/1931364236304830675
- We just launched our biggest update yet. Meet Higgsfield Speak â€” the fastest way to make motion-driven talking videos. Pick a style, choose an avatar, type a script. We do the rest â€” cinematic motion, voice, emotion. Comment Speak to get the full guide + promo code in the DM. https://x.com/higgsfield_ai/status/1930686472845455417
- Nando de Freitas on X: "RT @_akhaliq: ByteDance presents APT2 Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation https://t.co/yBDâ€¦" / X
- <https://x.com/NandoDF/status/1933266267663634465>
- AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation Â· <https://alignhuman.github.io/>
- SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting Â· <https://ziqiaopeng.github.io/synctalk++/>
- Edge AI Innovation: Real-Time Pose Detection | Dell Â· <https://www.dell.com/en-us/blog/edge-ai-innovation-real-time-pose-detection/>
- GASP Â· <https://microsoft.github.io/GASP/>
### May 2025

- GeomHair- Reconstruction of Hair Strands from Colorless 3D Scans Â· <https://seva100.github.io/GeomHair>
- Co3 - Co Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion Â· <https://mattie-e.github.io/Co3/>
- GUAVA - Project page - GUAVA: Generalizable Upper Body 3D Gaussian Avatar Â· <https://eastbeanzhang.github.io/GUAVA/>
- New sota open-source depth estimation: Marigold IID ğŸŒ¼ &gt; normal maps, depth maps of scenes &amp; faces &gt; get albedo (true color) and BRDF (texture) maps of scenes, they even release a depth-to-3D printer format demo ğŸ˜® link to all models and demos on the next one â¤µï¸  https://x.com/mervenoyann/status/1923318140965990814
- Marigold Computer Vision - a prs-eth Collection Â· <https://huggingface.co/collections/prs-eth/marigold-computer-vision-6669e9e3d3ee30f48214b9ba>
- TeGA: Texture Space Gaussian Avatars for High-Resolution DynamicHead Modeling https://syntec-research.github.io/UVGA/
- ğŸ¥ @higgsfield_ai Hollywood-Level Videos from a Single Image Uses 50+ pro-level camera moves â€” from bullet time to crash zooms, robo arms, and FPV chases â€” to turn static images into cinematic videos Some beutiful examples.. ğŸ§µ 1/n - 3D Rotation The subject or product spins https://x.com/rohanpaul_ai/status/1922241875089543546
- It's fun to see all the new uses cases coming up with this latest References update. Feels like Christmas but every day. Here is zero-shot novel view synthesis for people and characters, works straight out of the box. https://x.com/c_valenzuelab/status/1922656353354412332
- EVA: Expressive Virtual Avatars from Multi-view Videos Â· <https://vcai.mpi-inf.mpg.de/projects/EVA/>
- New sota open-source depth estimation: Marigold IID ğŸŒ¼ &gt; normal maps, depth maps of scenes &amp; faces &gt; get albedo (true color) and BRDF (texture) maps of scenes, they even release a depth-to-3D printer format demo ğŸ˜® link to all models and demos on the next one â¤µï¸ https://x.com/mervenoyann/status/1923318140965990814
- What if you could take a 2D video call and make it feel like youâ€™re really there? Google Beam, our new AI-first video communication platform, does just that â€” using a state-of-the-art AI video model to transform 2D video streams into a realistic 3D experience. #GoogleIO https://x.com/Google/status/1924875328037466302
- CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesishttps://fraunhoferhhi.github.io/cgs-gan/
- GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians | Shenhan Qian Â· <https://shenhanqian.github.io/gaussian-avatars>
- DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations Â· <https://ziqiaopeng.github.io/dualtalk/>
- BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading Â· <https://jonathsch.github.io/becominglit/>
- HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters Â· <https://hunyuanvideo-avatar.github.io/>
- Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction Â· <https://simongiebenhain.github.io/pixel3dmm/>
- DreamDance Â· <https://kebii.github.io/DreamDance/>
- LoRAvatar Â· <https://starc52.github.io/publications/2025-05-28-LoRAvatar/>
- SpatialScore (tangential) Â· <https://haoningwu3639.github.io/SpatialScore/>
- ChatHuman Â· <https://chathuman.github.io/>
- OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers Â· <https://ziqiaopeng.github.io/OmniSync/>
- HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation Â· <https://kkakkkka.github.io/HunyuanPortrait/>
### April 2025

- MoCha: Towards Movie-Grade Talking Character Synthesis  https://arxiv.org/pdf/2503.23307v1
- with Hybrid Guidance https://arxiv.org/pdf/2504.01724
- Segment Any Motion in Videos https://motion-seg.github.io/
- "Reconstructing dynamic 3D scenes (4D geometry) from single videos struggles with motion and often requires complex methods or specialized data. Sora3R solves this by leveraging large video diffusion models' learned understanding of motion and structure to directly generate 4D https://x.com/rohanpaul_ai/status/1906536678032109698
- DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance https://grisoon.github.io/DreamActor-M1/
- DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness https://ruiningli.com/dso
- Deep Imagination Research | NVIDIA https://research.nvidia.com/labs/dir/akd/
- MoCha: Towards Movie-Grade Talking Character Synthesis https://congwei1230.github.io/MoCha/
- "Meta just announced MoCha, an AI model that produces movie-grade talking/singing character animations from speech and text It even enables multi-character conversations with turn-based dialogue generation https://x.com/rowancheung/status/1907304629060198611
- OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication https://humanaigc.github.io/omnitalker/
- How Tavus is helping to make AI videos feel like real conversations https://ai.meta.com/blog/tavus-real-feeling-ai-videos-llama/
- AI Avatars Escape the Uncanny Valley | Andreessen Horowitz Â· <https://a16z.com/ai-avatars/>
- DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion Â· <https://cv.maxi.su/DiTaiListener/>
- "@AgentOpsAI 13/ Create Multilingual 2D Digital Humans for Enterprise Hosted by: Rochelle Pereira, Sr. Director of Engineering, NVIDIA Ragav Venkatesan, Principal Software Engineer, NVIDIA Learn about NVIDIA NIMÄâ€Â¢ microservices for secure, high-performance AI deployment across various" / X https://x.com/AtomSilverman/status/1907898487154626722
- "Alibaba just released LAM on Hugging Face Large Avatar Model for One-shot Animatable Gaussian Head https://x.com/_akhaliq/status/1910259092972589432
- Scene-Centric Unsupervised Panoptic Segmentation Â· <https://visinf.github.io/cups/>
- FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis Â· <https://fantasy-amap.github.io/fantasy-talking/>
- DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting Â· <https://jzr99.github.io/DNF-Avatar/>
- SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians Â· <https://nlml.github.io/sheap/>
- MediaPipe Face Landmark Â· <https://face-landmark-tracking.netlify.app/>
- Iâ–²N CURTIS on X: "Face tracking + 3D model morph targets Built with Bolt Rendered Using Three.js Powered by MediaPipe Try the demo here: https://t.co/y273B9tFDY Bolt's new mascot maybe ğŸ˜… clone the project -&gt; https://t.co/DOm5CHnlFf" / X
- <https://x.com/XRarchitect/status/1909730663877800030>
- InstantCharacter - a Hugging Face Space by InstantX Â· <https://huggingface.co/spaces/InstantX/InstantCharacter>
- Image segmentation using Gemini 2.5 Â· <https://simonwillison.net/2025/Apr/18/gemini-image-segmentation/>
- SEGA: Drivable 3D Gaussian Head Avatar from a Single Image Â· <https://sega-head.github.io/>
- AvatarFX is a cutting-edge AI platform for interactive storytelling, empowering users to bring characters to life by simply uploading an image and selecting a voice.  https://character-ai.github.io/avatar-fx/
- "HeyGen is using gpt-image-1 to enhance avatar creation, specifically improving avatar editing within the platform. https://x.com/OpenAIDevs/status/1915097077878530334
- "Character AI unveiled an AI platform called AvatarFX It allows users to create long-form, coherent videos of talking avatars from just a single reference photo and voice selection Launching soon to CAI+ subscribers! https://x.com/rowancheung/status/1914930092632928655
- "California-based Play AI released an AI voice changer The tool allows users to change their voice into ANYONE else's with just 10 seconds of audio Accessible on mobile, it even preserves the same voice and tone of the original recording! https://x.com/rowancheung/status/1914567423916638560
- "My AI avatar just hit 100,000 followers. The videos you see below aren't actually me, it's an AI avatar of my face/voice. Despite this, each of these videos went viral with millions of views across platforms. Some raw thoughts on this experiment: 1) People don't care it's https://x.com/rowancheung/status/1914706445817233507
- "We just dropped a new SoTA lipsync model on @FAL: Hummingbird-0 Available now as a research preview, it's the most accurate zero-shot lipsync model weâ€™ve tested, open or closed source. https://x.com/heytavus/status/1915435703833641231
- "Alibaba just announced Uni3C on Hugging Face Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation https://x.com/_akhaliq/status/1914619143925432338
- IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos Â· <https://y-u-a-n-l-i.github.io/projects/IM-Portrait/>
- KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution Â· <https://antonibigata.github.io/KeySync/>
- Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction https://simongiebenhain.github.io/pixel3dmm/
- Rowan Cheung on X: "Higgsfield AI introduced Iconic Scenes! The feature re-creates famous movie scenes with a different subject using just a single selfie from the user Available to try on the Higgsfield AI website https://t.co/EqaMiW4DPL" / X
- <https://x.com/rowancheung/status/1917095368073830652>
- Character.AI unveils AvatarFX, an AI video model to create lifelike chatbots | TechCrunch https://techcrunch.com/2025/04/22/character-ai-unveils-avatarfx-an-ai-video-model-to-create-lifelike-chatbots/
### March 2025

- Remade-AI/Rotate Â· Hugging Face Â· <https://huggingface.co/Remade-AI/Rotate>
- motimalu/wan-flat-color-v2 Â· Hugging Face Â· <https://huggingface.co/motimalu/wan-flat-color-v2>
- MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice Â· <https://magicinfinite.github.io/>
- Motion Anything Â· <https://steve-zeyu-zhang.github.io/MotionAnything/>
- Move AI on X: "Introducing Gen 2 spatial motion: 3D motion capture, full-body dynamics, joint torques, ground reaction forces, advanced motion retargeting, motion prediction. Works on AI video, phones, cinema cams, stadia. Launched at Movement Day, BAFTA London. https://t.co/2PKRJhzbZO" / X
- <https://x.com/MoveAI_/status/1899489539380617454>
- AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models https://kwanyun.github.io/AnyMoLe_page/
- AK on X: "Tencent just announced a â€‹30x acceleration in model generation speed across the entire Hunyuan3D 2.0 family, reducing the processing time from 30 seconds to just â€‹1 second, available on Hugging Face https://t.co/9BwipKQKR7" / X
- <https://x.com/_akhaliq/status/1902199977096499424>
- Animating the Uncaptured Â· <https://marcb.pro/atu/>
- AK on X: "Roblox just released Cube on Hugging Face A Roblox View of 3D Intelligence https://t.co/yMwmmlmrVU" /
- <https://x.com/_akhaliq/status/1902560381370839524>
- Rowan Cheung on X: "Stability AI unveiled Stable Virtual Camera, a new diffusion model It transforms single images into 3D videos with 14 dynamic camera paths Currently in research preview under a non-commercial license! https://t.co/MKNPlA1P4m" / X
- <https://x.com/rowancheung/status/1902250078913360345>
- RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars Â· <https://gapszju.github.io/RGBAvatar/>
- GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior Â· <https://silence-tang.github.io/gaussian-ip/>
- DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis Â· <https://freedomgu.github.io/DiffPortrait360/>
- MusicInfuser: Making Video Diffusion Listen and Dance Â· <https://susunghong.github.io/MusicInfuser/>
- HeadStudio Â· <https://zhenglinzhou.github.io/Zero-1-to-A/>
- ImaginTalk Â· <https://imagintalk.github.io/>
- ReCamMaster: Camera-Controlled Generative Rendering from A Single Video Â· <https://jianhongbai.github.io/ReCamMaster/>
- Paper page - LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds Â· <https://huggingface.co/papers/2503.10625>
- TaoAvatar Â· <https://pixelai-team.github.io/TaoAvatar/>
- NeRSemble Benchmark Â· <https://kaldir.vc.in.tum.de/nersemble_benchmark/>
- NeRSemble Benchmark Â· <https://kaldir.vc.in.tum.de/nersemble_benchmark/>
- Bilawal Sidhu on X: "The â€œmetaverseâ€ is a rather empty place without 3d content â€” generative AI gives us the means to populate it. Hereâ€™s a few things that have blown my mind lately â€” and why I think Roblox might be the perfect sandbox for these primitives coming together. https://t.co/xdev08JMkH" / X
- <https://x.com/bilawalsidhu/status/1902518941580234891>
- Hunyuan on X: "We are thrilled to announce a â€‹30x acceleration in white model generation speed across the entire Hunyuan3D 2.0 family, reducing the processing time from 30 seconds to just â€‹1 second. https://t.co/wYWSKVvj2e" / X
- <https://x.com/TXhunyuan/status/1902195164182888775>
- Robloxâ€™s new AI model can generate 3D objects | The Verge https://www.theverge.com/news/630977/roblox-cube-3d-objects-mesh-ai-text-prompt
- HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting Â· <https://kh129.github.io/hogs/>
- HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation https://kkakkkka.github.io/HunyuanPortrait/
- KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation https://antonibigata.github.io/KeyFace/
- ByteDance/InfiniteYou Â· Hugging Face Â· <https://huggingface.co/ByteDance/InfiniteYou>
### February 2025

- WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction
- TLDR: Gaussian full body Avatars using Score Distillation Sampling in both Canonical and Posed Space. Â· <https://wyiguanw.github.io/WonderHuman/>
- ByteDance ğŸ¶ has just released OmniHuman-1, and it's insanely good. It takes a single image and audio to produce these results.
- TLDR: Video Diffusion Model using a Multi-Modal Diffusion Transformer (MMDiT), trained with flow matching and using a ton of data. Â· <https://omnihuman-lab.github.io/>
- Bilawal Sidhu on X: "â€œBut AI will never get good at motion and physicsâ€ VideoJAM: hold my beerâ€¦ *Introduces motion priors, outperforms Runway, Sora &amp; Kling* Oh and works on any video model with minimal tweaks https://t.co/oSBT3f8t8p" / X - https://x.com/bilawalsidhu/status/1886874059503452234
- FaceXBench Â· <https://kartik-3004.github.io/facexbench/>
- Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics - https://dress-1-to-3.github.io/
- 2501.18801 - https://arxiv.org/pdf/2501.18801
- 2502.02465 - https://arxiv.org/pdf/2502.02465
- Rohan Paul on X: "This guy is on a Zoom call, but not actually in front of the camera. And you can also run it with text to speech open-source model trained on your own voice, written by a language model listening to the other user's voice input. https://t.co/BHyrgvVkVW" / X - https://x.com/rohanpaul_ai/status/1886757493998805226
- HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation Â· <https://agnjason.github.io/HumanDiT-page/>
- EventEgo3D++: 3D Human Motion Capture from a Head Mounted Event Camera https://eventego3d.mpi-inf.mpg.de/
- DiffListener: Discrete Diffusion Model for Listener Generation Â· <https://siyeoljung.github.io/DiffListener/>
- GAS: Generative Avatar Synthesis from a Single Image Â· <https://humansensinglab.github.io/GAS/>
- Meta presents: Pippo : High-Resolution Multi-View Humans from a Single Image Generates 1K resolution, multi-view, studio-quality images from a single photo in a one forward pass https://x.com/arankomatsuzaki/status/1889515688647373113
- Pippo: High-Resolution Multi-View Humans from a Single Image h
- Got a few selfies? Turn them into a moving, talking Selfie Avatar in minutes. ğŸ“¸ Upload your photos ğŸ“ Enter a prompt (get creative!) ğŸ™ï¸ Record a short voiceover And boom - your Selfie Avatar video, delivered to your inbox. Give it a try for free: https://x.com/synthesiaIO/status/1889302506401849501
- Pika AI: Pikadditions [Bring Your Video Imagination to Life] Â· <https://pikartai.com/pikaddition/>
- "Alibaba strikes again. Full-body swap anyone in a video with just a photo reference. Whatâ€™s wild to me is that this tech completely bypasses the 3d pipeline (i.e. what Wonder Dynamics does to accomplish similar output) and yet looks so damn good. Basically viggle on steroids." / X https://x.com/bilawalsidhu/status/1890535455600369687
- Brivael on X: "We are claiming SOTA for AI Avatar, but the ultimate test is big face. We don't use post process or blurring hacks to hide misery. 5 videos. Same script. Generate with Argil ğŸ‘‡ https://t.co/DsFEhVCvBu" / X
- <https://x.com/BrivaelLp/status/1890435559127986463>
- Min Choi on X: "2. Goku+: Product and Human Interaction https://t.co/KGtK4DPxTw" / X
- <https://x.com/minchoi/status/1890074266244395495>
- AK on X: "ByteDance presents Phantom Subject-consistent video generation via cross-modal alignment https://t.co/9dV7QA0DD6" / X
- <https://x.com/_akhaliq/status/1892073250974216476>
- Paper page - Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator Â· <https://huggingface.co/papers/2502.19204>
- "The internet going wild with the microwave AI filter -- prolly because it's pure nightmare fuel ğŸ˜­ https://x.com/bilawalsidhu/status/1892789671672918425
- SkalskiP on X: ""new" SOTA object detector, and it's NOT YOLOv12 D-FINE is a model released 3 months ago under Apache-2.0 license; I have no idea how it flew under my radar @onuralpszr thanks for adding it to leaderboard leaderboard link: https://t.co/9z9ImuoChZ â†“ more about architecture https://t.co/Q8i4aURQ4n" / X
- <https://x.com/skalskip92/status/1892497124534747193>
- NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis https://nerf-3dtalker.github.io/NeRF-3Dtalker/
- CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image Â· <https://sites.google.com/view/cast4>
- "Awesome research from ByteDance continues. Current methods of Subject-to-video merges text prompts and reference images to produce consistent videos, yet many approaches fail to preserve subject fidelity. This new research Phantom merges text and reference-image features in a  https://x.com/rohanpaul_ai/status/1894000198210490440
- Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars https://tobias-kirschstein.github.io/avat3r/
- BodyGen: Advancing Towards Efficient Embodiment Co-Design Â· <https://genesisorigin.github.io/>
- "This could be huge for AI storytelling. I've tried so many AI lipsync tools and have never been satisfied. This is my first try with @hedra_labs Character-3 and it's blown me away. It was so easy. I uploaded a track from @SunoMusic and an image from @midjourney and it just https://x.com/TomLikesRobots/status/1898009257598980587
- InsTaG: Learning Personalized 3D Talking Head from Few-Second Video Â· <https://fictionarry.github.io/InsTaG/>
- Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation Â· <https://ltt-o.github.io/Kiss3dgen.github.io/>
- UnCommon Objects in 3D Â· <https://uco3d.github.io/>
- Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture Â· <https://xuanchenli.github.io/TexTalk/>
### January 2025

- "HOLY SHIT - generate 3D mesh from a single image in LESS THAN A SECOND ğŸ¤¯ Â· <https://x.com/reach_vb/status/1877100242534912068>
- AI at Meta on X: "Released by @RealityLabs Research at #ECCV2024, Nymeria is a large-scale multimodal egocentric dataset for full-body motion understanding with potential applications in VR/MR headsets, smart glasses and more. More on this work + access to the dataset â¡ï¸ https://t.co/V9vOBt46u8 https://t.co/w8eXEWcJeL" / X - https://x.com/AIatMeta/status/1877058463706325326
- "Great Open-source model and paper from ByteDance Face dubbing made simple - from sound to lips in one smooth move. Skip the motion middleman, direct audio-to-lip sync in latent space eliminates the middleman for better expressions. LatentSync introduces end-to-end lip sync Â· <https://x.com/rohanpaul_ai/status/1876213656645763098>
- Anyone want the script to run Moondream 2b's new gaze detection on any video? : r/LocalLLaMA Â· <https://www.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/>
- EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion Â· <https://arxiv.org/abs/2501.13452>
- EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Â· <https://humanaigc.github.io/emote-portrait-alive-2/>
- TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation Â· <https://lkjkjoiuiu.github.io/TalkingEyes_Home/>
- SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces Â· <https://vrroom.github.io/synthlight/>
- Dynamic Face
- High-Quality and Consistent Video Face Swapping using Composable 3D Facial Priors Â· <https://dynamic-face.github.io/>
- nvlabs.github.io/FoundationStereo/ - https://nvlabs.github.io/FoundationStereo/
- "Video Depth Anything is out! ğŸš€ Real-time inference for arbitrarily long videos with temporal and spatial consistency. Built on the excellent Depth Anything v2 (for images), by "simply" replacing the head and adjusting the loss for temporal consistency. Videos from the project Â· <https://x.com/pcuenq/status/1881978412270829728>
- Relightable Full-body Gaussian Codec Avatars Â· <https://neuralbodies.github.io/RFGCA/>

## 2024

### December 2024

- "Introducing Hailuo I2V-01-Live: Transform Static Art into Dynamic Masterpieces Live is our latest addition to the I2V family, designed to revolutionize how 2D illustrations come to life. With enhanced smoothness and vivid motion, this model lets your characters move, speak, and Â· <https://x.com/Hailuo_AI/status/1863961575574622662>
- "Our new work PSHuman reconstructs a detailed 3D human mesh from a single-view image in ~1min. Codes have already been released! Welcome to try it! Project page: Â· <https://x.com/YuanLiu41955461/status/1863149385170973135>
- "Wav2lip can finally rest in peace. Being able to retarget the facial performance of characters in *existing* live action &amp; CG video makes Act-One an extremely useful tool for all types of creators. Nicely done Runway! Â· <https://x.com/bilawalsidhu/status/1865067246038520157>
- "ğŸš€ Introducing LayerDecomp: our latest generative framework for image layer decomposition, which can output photorealistic clean backgrounds and high-quality transparent foregrounds, faithfully preserving visual effects like shadows and reflections. Our key contributions include Â· <https://x.com/yuyinzhou_cs/status/1864500829040095737>
- "Recent video depth models thrive on large-scale annotated datasetsâ€”but what if theyâ€™re unavailable? Introducing Buffer Anytime: a zero-shot framework using image priors to predict video depth and normals. Trained exclusively on unannotated videos, it achieves surprisingly smooth Â· <https://x.com/zfkuang1/status/1862594009543450998>
- "The new Viggle 3.0 model is a big improvement, less of the viggle wiggle and more definition. We are getting loads of control in AI gen this weekend. Bravo team @ViggleAI Â· <https://x.com/Uncanny_Harry/status/1865379507923824799>
- "LineGS : 3D Line Segment Representation on 3D Gaussian Splatting Chenggang Yang, Yuang Shi, Wei Tsang Ooi tl;dr: 3DGS tend to be dense along the edges -&gt; so they can help filtering &amp; post processing the lineclouds. Â· <https://x.com/ducha_aiki/status/1867556550661075243>
- "Excited to introduce Proc-GSğŸš€! A new pipeline integrating procedural modeling with 3D-GS to accelerate 3D building creation and infinite city generation with high flexibility and photo-realistic visual quality. -Project: Â· <https://x.com/liyixxxuan/status/1866841261157781710>
- "ğŸ“ We introduce SAMa! A material selection and segmentation model on 3D models in any format (3DGS, NeRF, Mesh). Given a user click, we propose to select all regions on an objects with the same material. We can also do segmentation in under a minute: Â· <https://x.com/vdeschaintre/status/1866142914885382152>
- "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos Contributions: 1) A framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video. 2) DynaDUSt3R, a method that takes a pair of frames Â· <https://x.com/janusch_patas/status/1867511260872683921>
- "The future of sports broadcasting is volumetric. Combine this 3d tech with advances in object detection &amp; tracking, and you can tap a button to follow the ball or even a specific player. Split screen + stats + social for the full experience. Cant wait! Â· <https://x.com/bilawalsidhu/status/1867704947799986608>
- Background Removal Arena - a Hugging Face Space by bgsys Â· <https://huggingface.co/spaces/bgsys/background-removal-arena>
- "We built a AI model arena to rank the top background removal tools based on community votes. Photoroom wouldn't be here without open source so I am very proud of this opportunity to bring a transparent, open, reliable resource for everyone. On @huggingface. cc @julien_c Â· <https://x.com/matthieurouif/status/1866813819676205131>
- "Ah! This was an experiment worth doing! I used kolorize @PhotoColorizer to normalize the color palette in the trailer for 'The Name and the Shadow'. Now, it all looks like a coherent whole, cut from the same movie, rather than a bunch of wildly different clips. Turn ğŸ”Š up! Â· <https://x.com/markjeffrey/status/1867367669756047563>
- "ElevenLabs x @hedra_labs Voice Your Character Competition. With our new Voice Design feature now in Hedra, you can create unique voices from a simple text prompt along with lip-synced character videos. To celebrate, weâ€™re hosting a competition with $4k in prizes. Â· <https://x.com/elevenlabsio/status/1866180148430774734>
- "The new Runway Act-One update is wild I used the first 30 seconds from Johnny Harris's latest video to drive multiple AI-generated versions and changed his voice to a female one with ElevenLabs. 2025 is going to be out of this world ğŸš€ Â· <https://x.com/maxescu/status/1865492523986063739>
- "Check out this Stereo4D paper from Google DeepMind. It's a pretty clever approach to a persistent problem in computer vision -- getting good training data for how things move in 3D. The key insight is using VR180 videos -- those stereo fisheye videos we launched back in 2017 for Â· <https://x.com/bilawalsidhu/status/1868298708841902172>
- [2412.09296v1] GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with Rhythmic Poses and Realistic Expression Â· <https://arxiv.org/abs/2412.09296v1>
- "Excited to share StableAnimator, an open-source human image animation model that excels in identity preservation! It transforms reference images into stunning, pose-driven animations with remarkable fidelity. Â· <https://x.com/bbldCVer/status/1866020964271800830>
- "New paper presents Exbody2, a whole-body tracking framework enabling humanoid robots to mimic dynamic, human-like motions (e.g., running, dancing) with stability. Trained via RL in simulation and transferred to real robots, it outperforms prior methods. Â· <https://x.com/TheHumanoidHub/status/1869109219196375417>
- Viggle Adjacentâ€¦ but different methodsâ€¦ converging.
- "Ed Catmull (Pixar founder) just joined Odyssey's board as they reveal their first 3D breakthrough: Instantly turning any 2D image into an editable 3D world. World Labs isn't alone anymore. Here's the TL;DR without the hype ğŸ§µ Â· <https://x.com/bilawalsidhu/status/1869472246131269878>
- "Meta releases Apollo An Exploration of Video Understanding in Large Multimodal Models a family of state-of-the-art video-LMMs Â· <https://x.com/_akhaliq/status/1868535608370708643>
- Prompt Depth Anything Â· <https://promptda.github.io/>
- LeviTor: 3D Trajectory Oriented <br> Image-to-Video Synthesis Â· <https://ppetrichor.github.io/levitor.github.io/>
- "ğŸš€Turn Single Image into 3D HumanğŸš€ #GeneMAN is a generalizable single-image 3D human reconstruction framework that turns in-the-wild images into high-quality 3D humans with ease ğŸ”—Project: Â· <https://x.com/liuziwei7/status/1863965769979285894>
- "Excited to announce our new work SAT-HMR ğŸ¥³ With a better balance of performance and efficiency, it's the best real-time 3D multi-person mesh estimation model. Check it out here: Â· <https://x.com/XiaoxuanMa_/status/1870494395822395779>
- Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion Â· <https://kakituken.github.io/affordance-any.github.io/>
- "DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding TL;DR: DINO-X Pro: sota model with enhanced perception capabilities for various scenarios; DINO-X Edge: model optimized for faster inference speed and better suited for deployment on edge devices Â· <https://x.com/Almorgand/status/1872598770179035191>
- merve on X: "ViTPose -- best open-source pose estimation model just landed to @huggingface transformers ğŸ•ºğŸ»ğŸ’ƒğŸ» See how to use on the next one â¤µï¸ https://t.co/lQYvT064Wu" / X - https://x.com/mervenoyann/status/1877360767478952012
### November 2024

- "Look at the realism here! Hot take: This blows most 'old-school' CG humans out of the water. ğŸ˜® "But you have no control".. it's getting there! Using 'Facepoke' as input for Runway's video-to-video, it's clear we're entering a new era of creation. #AIArt #GenerativeAI Â· <https://x.com/MartinNebelong/status/1843409578438107519>
- EchoMimic - a Hugging Face Space by fffiloni
- Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning Â· <https://huggingface.co/spaces/fffiloni/EchoMimic>
- Learning Inclusion Matching for Animation Paint Bucket Colorization Â· <https://ykdai.github.io/projects/InclusionMatching>
- PF3plat : Pose-Free Feed-Forward 3D Gaussian Splatting Â· <https://cvlab-kaist.github.io/PF3plat/>
- LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias Â· <https://haian-jin.github.io/projects/LVSM/>
- NoPoSplat
- Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images Â· <https://noposplat.github.io/>
- MeshRet
- Skinned Motion Retargeting with Dense Geometric Interaction Perception Â· <https://abcyzj.github.io/MeshRet/>
- "Weâ€™re excited to see creators like Ken @ODonnellKen push the boundaries of whatâ€™s possible using our markerless motion capture technology! Whether youâ€™re an indie studio or a creative team, Move AI empowers you to bring your ideas to life at scale. ğŸŒâœ¨ #MoveAI Â· <https://x.com/MoveAI_/status/1848696832378966359>
- X-Portrait 2: Highly Expressive Portrait Animation Â· <https://byteaigc.github.io/X-Portrait2/>
- "DimensionX Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion Â· <https://x.com/_akhaliq/status/1854917732174573583>
- "AdvancedLivePortrait-WebUI Dedicated gradio based WebUI for ComfyUI-AdvancedLivePortrait edit the facial expression from the image Â· <https://x.com/_akhaliq/status/1854222095477318096>
- StdGEN: Semantic-Decomposed 3D Character Generation from Single Images Â· <https://stdgen.github.io/>
- [2411.04632v1] Improved Multi-Task Brain Tumour Segmentation with Synthetic Data Augmentation Â· <https://arxiv.org/abs/2411.04632v1>
- "We've released our paper "Generating 3D-Consistent Videos from Unposed Internet Photos"! Video models like Luma generate pretty videos, but sometimes struggle with 3D consistency. We can do better by scaling them with 3D-aware objectives. 1/N page: Â· <https://x.com/gene_ch0u/status/1859419280468594972>
- "ğŸ’«ğŸª‘Introducing IKEA Manuals at Work: The first multimodal dataset with extensive 4D groundings of assembly in internet videos! We track furniture partsâ€™ 6-DoF poses and segmentation masks through the assembly process, revealing how parts connect in both 2D and 3D space. With Â· <https://x.com/yunongliu1/status/1858951328980251065>
- "3D Gaussian Splats are cool, but they're static (Part 37). GPS-Gaussian+ can render high-resolution 3D scenes from 2 or more input images in real-time! Links â¬‡ï¸ Â· <https://x.com/dreamingtulpa/status/1861366917317218327>
- EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation Â· <https://antgroup.github.io/ai/echomimic_v2/>
- "Monocular depth estimation has gotten so good lately. It used to look like a blurry blobby flickering mess. Now it looks like a synthetic depth map youâ€™d export from a 3d tool Like putting on a fresh pair of glasses ğŸ¤“ Â· <https://x.com/bilawalsidhu/status/1862597954906870120>
- "How far are we from embodied intelligence? We first ask whether we can build a model to locate semantic concepts in 3D. Meet Find3DğŸ”: a model to locate any part of any 3D object based on any text query! ğŸ‘‰ Â· <https://x.com/ziqi__ma/status/1859634303296266536>
- "ğŸš€ Introducing CAT4D! ğŸš€ CAT4D transforms any real or generated video into dynamic 3D scenes with a multi-view video diffusion model. The outputs are dynamic 3D models that we can freeze and look at from novel viewpoints, in real-time! Be sure to try our interactive viewer! Â· <https://x.com/ChrisWu6080/status/1861964000646340630>
- "Stop watching videos, start interacting with worlds. Stoked to share CAT4D, our new method for turning videos into dynamic 3D scenes that you can move through in real-time! Â· <https://x.com/poolio/status/1861964631389950265>
- "Just read a neat AI paper called SAMURAI -- it takes SAM 2 (Meta's "segment anything" model) and makes it way better at tracking objects in videos. Basic problem is SAM 2 gets confused when things move fast or there's a crowd of similar objects (big problem for VFX and video Â· <https://x.com/bilawalsidhu/status/1860348056916369881>
- SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory - https://yangchris11.github.io/samurai/
### October 2024

- TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio-Motion Embedding and Diffusion Interpolation Â· <https://pantomatrix.github.io/TANGO/>
- ğŸ¨ CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos Â· <https://huggingface.co/spaces/facebook/cotracker>
- CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos Â· <https://cotracker3.github.io/>
- I had to test how Gen-3â€™s video-to-video would perform using a Viggle output. Have any of you done any interesting with vid2vid? https://t.co/7pxTJKwGagâ€
- <https://twitter.com/8bit_e/status/1844435081869525489>
- (4) Halim Alrasihi on X: "This is Hollywood grade lip-syncing: Here is how to create an AI avatar with accurate and realistic Lip-Sync. This might be currently the best tool for video-to-video right now. Only 4 steps are needed: https://t.co/TGYuRLMW0g" / X - https://twitter.com/HalimAlrasihi/status/1839310216602788103
- (4) enigmatic_e on X: "Now the BTS. I recorded myself lip-syncing the song to transfer my facial movements onto images and videos. The parts where thereâ€™s other movements, I used Runway and Lumalabs to get the images to move and used liveportrait after to get the mouth movements. #kanye #ye #comfyui https://t.co/4nFAA9TsSc" / X - https://twitter.com/8bit_e/status/1839732374336090625
### September 2024

- "Finally got around to really dig deeper into LivePortrait. Â· <https://x.com/8bit_e/status/1833250941006491876>
- (4) enigmatic_e on X: "A little experiment I worked on using LivePortrait, Viggle, and ComfyUI. Will be doing a breakdown soon! https://t.co/sTWSjGXKaz" / X - https://twitter.com/8bit_e/status/1836845874468839612?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1836845874468839612%7Ctwgr%5Ec2ca01bc19492dfac671858b9f413a4357e47a78%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F09%2F21%2Fai-news-51-week-ending-09-20-2024-with-executive-summary-top-80-links-and-helpful-visuals%2F
### August 2024

- "Shape of Motion 4D Reconstruction from a Single Video Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective Â· <https://twitter.com/_akhaliq/status/1814156712175083539>
- "Generalizable Implicit Motion Modeling for Video Frame Interpolation Motion modeling is critical in flow-based Video Frame Interpolation (VFI). Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps Â· <https://twitter.com/_akhaliq/status/1811583979067207960>
- Model a full head from a single portrait image
- "ğŸš¨Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360Â° ğŸŒŸğğ«ğ¨ğ£: Â· <https://twitter.com/_vztu/status/1819838124161200223>
### July 2024

- MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices | AI Research Paper Details - https://www.aimodels.fyi/papers/arxiv/mobileportrait-real-time-one-shot-neural-head
- [2407.05712] MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices - https://arxiv.org/abs/2407.05712
- This Free AI Video Tool Brings Characters to Life - YouTube - https://www.youtube.com/watch?v=cucaEEDYmsw&t=2s
- [2307.09906] Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation - https://arxiv.org/abs/2307.09906
- MCNet: Talking Head Video Generation - https://goatstack.ai/topics/mcnet-talking-head-video-generation-gsgchf
- Live Portrait - a Hugging Face Space by KwaiVGI - https://huggingface.co/spaces/KwaiVGI/LivePortrait
- 2407.03168 - https://arxiv.org/pdf/2407.03168
- Efficient Portrait Animation with Stitching and Retargeting Control - https://liveportrait.github.io/
- ConceptExpress: Unsupervised Concept Extraction (UCE): We focus on the unsupervised problem of extracting multiple concepts from a single image. Given an image that contains multiple concepts, we aim to harness a frozen pretrained diffusion model to automatically learn the conceptual tokens. Using the learned conceptual tokens, we can regenerate the extracted concepts with high quality. Â· <https://haoosz.github.io/ConceptExpress/>
- Researchers leverage shadows to model 3D scenes, including objects blocked from view | MIT News | Massachusetts Institute of Technology Â· <https://news.mit.edu/2024/researchers-leverage-shadows-model-3d-scenes-blocked-objects-0618>
- "Cool demo by @mervenoyann for real-time object tracking with RT-DETR Â· <https://twitter.com/fdaudens/status/1811029049638011000>
### June 2024

- Florent Daudens on X: "New vision model from @Microsoft, Florence-2 - Can perform various tasks: object detection, grounding, segmentation, OCR - 200M and 800M models https://t.co/Wp8LMjkM0M" / X - https://twitter.com/fdaudens/status/1803427363746717866?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1803427363746717866%7Ctwgr%5E1e807c2a883c66c70805660a846157e2d1ee1440%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F21%2Fmicrosoft-ai-news-week-ending-06-21-2024%2F
- AK on X: "Depth Anything V2 This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more https://t.co/s7TNDJIDvQ" / X - https://twitter.com/_akhaliq/status/1801432403665125738?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1801432403665125738%7Ctwgr%5E3b1555acae76e0fd9879fd4ccb60a782b4dc2a93%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F14%2Fmultimodality-news-week-ending-06-14-2024%2F
- Aran Komatsuzaki on X: "TikTok presents Depth Anything V2 Trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation model proj: https://t.co/KaOQauiOST abs: https://t.co/9HxIpsPWJJ https://t.co/aj9S1SKjzN" / X - https://twitter.com/arankomatsuzaki/status/1801435793254121798?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1801435793254121798%7Ctwgr%5E3b1555acae76e0fd9879fd4ccb60a782b4dc2a93%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F14%2Fmultimodality-news-week-ending-06-14-2024%2F
### May 2024

- Moving Object Segmentation: All You Need Is SAM (and Flow) - https://www.robots.ox.ac.uk/~vgg/research/flowsam/
- Amar Singh on X: "In honor of the playoffs, Iâ€™d like to showcase what weâ€™ve been working on here at Nexavision â€” a new way to generate basketball analytics through tracking with computer vision and AI: ğŸ§µ https://t.co/DgC3BBXr38" / X - https://twitter.com/AmarSVS/status/1793037268690579787
- Francis Engelmann on X: "I'll be at @iclr_conf in Vienna next week, presenting OpenNeRF (Open-Set 3D Neural Scene Segmentation) ğŸ“Paper https://t.co/9MUl3PnhbA ğŸ‘©â€ğŸ’»Code https://t.co/vLZMlx0iZr ğŸš€Project https://t.co/1R8qnW3QQv w/@fedassa @mapo1 @Mi_Niemeyer et al. Zero-shot unsupervised 3D segmentation: https://t.co/iWSAOzvU5r" / X - https://twitter.com/FrancisEngelman/status/1787133195973984749?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1787133195973984749%7Ctwgr%5E3a0929c0188ecefc08d3f027e68d0c98cfaa43c0%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F05%2F10%2Fopen-source-ai-news-week-ending-05-10-2024%2F
- Jim Fan on X: "What makes up the abstract concept of an apple? We read the word "apple" as a string, see 2D pictures online, 3D shape in real life, and moving apples in videos. We touch the apple, feel its geometry in our palms and texture through the rich tactile sensation on our fingers. Do https://t.co/2LzxYa4f3N" / X - https://twitter.com/DrJimFan/status/1793318771932995793?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1793318771932995793%7Ctwgr%5Ed9dcfb501f151309c4a242c5a48759e1425e6ad1%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F05%2F24%2Fmultimodality-news-week-ending-05-24-2024%2F
- Amar Singh on X: "In honor of the playoffs, Iâ€™d like to showcase what weâ€™ve been working on here at Nexavision â€” a new way to generate basketball analytics through tracking with computer vision and AI: ğŸ§µ https://t.co/DgC3BBXr38" / X - https://twitter.com/AmarSVS/status/1793037268690579787
- Reason3D - https://kuanchihhuang.github.io/project/reason3d/
- Jim Fan on X: "We've found a cozy home for our robots in the world of bits! RoboCasa: a place where robot arms, dogs, and humanoids can train safely for daily tasks in procedurally generated simulations. It's all open-source, check it out in Yuke's thread!" / X - https://twitter.com/DrJimFan/status/1795482053053669687?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1795482053053669687%7Ctwgr%5E4a203dcd9558e4536c5e7700b6612e6c7df68c5f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F06%2F02%2Frobotics-and-embodiment-news-week-ending-05-31-2024%2F
### April 2024

- Dreaming Tulpa ğŸ¥“ğŸ‘‘ on X: "SAM + Optical Flow = FlowSAM FlowSAM can discover and segment moving objects in a video and outperforms all previous approaches by a considerable margin in both single and multi-object benchmarks ğŸ”¥ https://t.co/qjaMQOYtlN https://t.co/mp2MTlehV6" / X - https://twitter.com/dreamingtulpa/status/1782083341883224462
- The Viggle AI Meme's Impact on Image-to-Video Awareness - Ethan B. Holland - https://ethanbholland.com/2024/04/08/the-viggle-ai-memes-impact-on-image-to-video-awareness/
- HeyGen on X: "[ğ—¡ğ—˜ğ—ª]âœ¨ğŸ¤³ğŸ½ URL â¡ï¸ UGC Meet Gail, Tim, Lukas, Daran, Lea, and Kim â€” HeyGen's newest UGC-style avatars! 1ï¸âƒ£ Input a URL (from Amazon, a Shopify store, etc.) 2ï¸âƒ£ Customize title, description, media, script, and avatar 3ï¸âƒ£ Create content in seconds for social media, emails, etc. https://t.co/DQCCM7f5VX" / X - https://twitter.com/HeyGen_Official/status/1780597444335575108
- A Visual Guide to Vision Transformers | MDTURP - https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html
- The latest HeyGen avatars - quick demo - Ethan B. Holland - https://ethanbholland.com/2024/04/26/the-latest-heygen-avatars-quick-demo/
- Jim Fan on X: "one day PhDs will animate every object around us with reinforcement learning to keep their thesis going https://t.co/A9ZNNKlmwP" / X - https://twitter.com/DrJimFan/status/1778819699062735350
### March 2024

- ViggleAI on X: "VIGGLE'S NEW MODEL IS HERE! Controllable video generation is our mission, and character is where we start. Animate any character just as you want, simply with a prompt, or by uploading a reference video featuring clear motion. Try Viggle for FREE: https://t.co/11cDhfqisU! https://t.co/hVWWyE4IOa" / X - https://twitter.com/ViggleAI/status/1768224856406131110?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1768224856406131110%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- HeyGen on X: "[NEW] â€” Avatar in Motion 1.0ğŸƒâ€â™‚ï¸âœ¨ Move your hands, make gestures, use unique tones of voice, and HeyGen will flawlessly track, translate, and lip-sync your video with any input text. Can't believe it? ğŸ‘‡Comment a script for what we should make Nik say &amp; weâ€™ll post the video! https://t.co/d1X75Z5m2S" / X - https://twitter.com/HeyGen_Official/status/1773119891068883240
- Image Animation Using Thin Plate Spline Motion Model - a Hugging Face Space by CVPR - https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model
- AniPortrait Â· <https://arxiv.org/abs/2403.17694>
- benjamin-paine/aniportrait Â· Hugging Face - https://huggingface.co/benjamin-paine/aniportrait
- Figure Status Update - OpenAI Speech-to-Speech Reasoning - YouTube - https://www.youtube.com/watch?v=Sq1QZB5baNw

## 2023

### August 2023

- NVIDIA Omniverse Audio2Face App - YouTube - https://www.youtube.com/playlist?list=PL3jK4xNnlCVfIJU10iQ_dd8J_5T_e0YTJ
- NVIDIA NIM | audio2face - https://build.nvidia.com/nvidia/audio2face
### July 2023

- Efficient Video Portrait Reenactment via Grid-based Codebook | ACM SIGGRAPH 2023 Conference Proceedings - https://dl.acm.org/doi/abs/10.1145/3588432.3591509
- AK on X: "Tracking Anything with Decoupled Video Segmentation paper page: https://t.co/xqfwTkf78V Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To https://t.co/6FSShhURHV" / X - https://twitter.com/_akhaliq/status/1700030823926280448?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700030823926280448%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- AI Mid-Term Outlook: Ethan Holland Presentation at the National Association of Broadcasters - Sept 20, 2023 - Ethan B. Holland - https://ethanbholland.com/2023/09/20/hello-world/
- AK on X: "Tracking Anything with Decoupled Video Segmentation paper page: https://t.co/xqfwTkf78V Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To https://t.co/6FSShhURHV" / X - https://twitter.com/_akhaliq/status/1700030823926280448?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700030823926280448%7Ctwgr%5Eec61b21c307f753813fc43c4b40832eddd43e735%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F10%2F06%2Fai-news-week-ending-10-6-2023%2F
- Nat Friedman on X: "But there's more: a few days ago, @Youssef_M_Nader, working with new segmentations from the Vesuvius Challenge segmentation team, was able to generate this image of stunning clarity and size: This is the revolution in digital papyrology that Dr. Seales has created. Nothing like https://t.co/oFyzYpsjRi" / X - https://twitter.com/natfriedman/status/1712475202008109526?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1712475202008109526%7Ctwgr%5E6d9331d3195dc5c357a8844c99e18e74f44f03a7%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F10%2F13%2Fai-news-2-week-ending-10-13-2023%2F
- Zhe Gan on X: "ğŸš€ğŸš€Introducing Ferret, a new MLLM that can refer and ground anything anywhere at any granularity. ğŸ“°https://t.co/gED9Vu0I4y 1âƒ£ Ferret enables referring of an image region at any shape 2âƒ£ It often shows better precise understanding of small image regions than GPT-4V (sec 5.6) https://t.co/yVzgVYJmHc" / X - https://twitter.com/zhegan4/status/1712345137983201716?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1712345137983201716%7Ctwgr%5Ed10d666cd7f2c97432d9d29aa4c1796f6c84c175%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F01%2F07%2Fai-news-12-week-ending-12-29-2023-with-executive-summary-and-top-9-stories%2F
- Unsupervised Object Segmentation - https://vlar-group.github.io/UnsupObjSeg.html
- AI Art Just Changed Forever - YouTube - https://www.youtube.com/watch?v=4-2dSRjErE4&t=4s
- 2311.02549 - https://arxiv.org/pdf/2311.02549
- Segmentation-Based Parametric Painting - https://manuelladron.github.io/semantic_based_painting/
- Magic Animates - Bring Your Photos to Dance - https://magicanimates.com/
- Aran Komatsuzaki on X: "Tracking Any Object Amodally Infers complete object boundaries, even when certain portions are occluded. proj: https://t.co/TaruN0WQ61 abs: https://t.co/JqsIRFQkFK https://t.co/4crKpIgfZv" / X - https://twitter.com/arankomatsuzaki/status/1737309414443438221?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737309414443438221%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- merve on X: "Think of an LLM that can find entities in a given image, describe the image and answers questions about it, without hallucinating âœ¨ Kosmos-2 released by @Microsoft is a very underrated model that can do that. Code snippet with transformers integration is in the next tweet ğŸ‘‡ https://t.co/tscJEygdz7" / X - https://twitter.com/mervenoyann/status/1737506720249782495?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737506720249782495%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- Lukas Hoyer on X: "ğŸš€Introducing SemiVL - a semi-supervised semantic segmentation method exploiting priors from vision-language models ğŸ“„Paper: https://t.co/bEOBn17vD5 ğŸ’»Code: https://t.co/4qnEHv2uZb ğŸ˜ŠTeam: @lukashoyer3, David J. Tan, @ferjadnaeem, Luc Van Gool, @fedassa #ETHZurich #Google https://t.co/4Ix4RTu60A" / X - https://twitter.com/lukashoyer3/status/1737837511433666699?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1737837511433666699%7Ctwgr%5Ed87b20be88e1b253892c61130e8e0f4dba140219%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2023%2F12%2F24%2Fai-news-11-week-ending-12-22-2023-with-executive-summary-and-top-7-stories%2F
- AK on X: "Bytedance announces DiffPortrait3D Controllable Diffusion for Zero-Shot Portrait View Synthesis paper page: https://t.co/lOpCdNXgUi given an unposed portrait image, diffportrait3d can synthesize plausible but consistent facial details with retained both identity and facial https://t.co/AODei632q9" / X - https://twitter.com/_akhaliq/status/1738247956019524048?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1738247956019524048%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- ByteDance Introduces MagicVideo-V2: A Groundbreaking End-to-End Pipeline for High-Fidelity Video Generation from Textual Descriptions - MarkTechPost - https://www.marktechpost.com/2024/01/16/bytedance-introduces-magicvideo-v2-a-groundbreaking-end-to-end-pipeline-for-high-fidelity-video-generation-from-textual-descriptions/
- Jim Fan: The next grand challenge for AI | TED Talk - https://www.ted.com/talks/jim_fan_the_next_grand_challenge_for_ai?subtitle=en
- OMG-Seg - https://lxtgh.github.io/project/omg_seg/
- sync.labs (YC W24) on X: "âš¡ launch + monetize your own AI dubbing app in &lt;30 mins drop a YouTube link + translate videos w/ perfect lip-sync curious? we open sourced the whole repo ğŸš€ try it out / clone it here ğŸ‘‡ https://t.co/39tWvYErqD" / X - https://twitter.com/synclabs_so/status/1752496443003371637?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1752496443003371637%7Ctwgr%5Ee9ade09f48f864c11e0a15bd405f1f88df483a97%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F02%2F07%2Fai-video-news-week-ending-02-02-2024%2F
- HeyGen on X: "Introducing NEW AI Avatars! ğŸ§‘â€ğŸ¤â€ğŸ§‘ ğŸ“New and Diverse Avatars ğŸ“Team Branding Kits enable you to maintain a consistent feel, ensuring a unified brand voice and appearance to keep your business messaging aligned. ğŸ“New Generative Outfits https://t.co/DJcX9xUJ08" / X - https://twitter.com/HeyGen_Official/status/1747697831207485541?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1747697831207485541%7Ctwgr%5E725844436ea18e7baf1e3b7157641de755fe0415%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F01%2F20%2Fai-video-news-week-ending-01-19-2024%2F
- pix2gestalt: Amodal Segmentation by Synthesizing Wholes - https://gestalt.cs.columbia.edu/
- Bo Wang on X: "ğŸ‰ The best way to start the week is to find out that our MedSAM is finally published today in Nature Communications! **Segment anything in medical images** Paper: https://t.co/CFKykvt07n arXiv: https://t.co/QOC9KaCg41 Data &amp; Code: https://t.co/r05Agl59xY MedSAM is the first https://t.co/2tuEOgtel8" / X - https://twitter.com/BoWang87/status/1749623699546788124
- SkalskiP on X: "REAL-TIME object detection WITHOUT TRAINING YOLO-World is a new SOTA open-vocabulary object detector that outperforms previous models in terms of both accuracy and speed. 35.4 AP with 52.0 FPS on V100. â†“ read more https://t.co/SoqFyEk41V" / X - https://twitter.com/skalskip92/status/1754916529672438173?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1754916529672438173%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Emad on X: "Pretty much, just describe to edit. In the sora technical paper they note that they apply SDEdit by @chenlin_meng et al fine - you can take that further through scene decomposition, segmentation, regional prompting &amp; more to autocorrect &amp; balance image, video, audio, etc etc https://t.co/C2k4vf2bkm" / X - https://twitter.com/EMostaque/status/1758854642635526625
- Gorden Sun on X: "Depth Anythingï¼šæŠŠå›¾ç‰‡å’Œè§†é¢‘è½¬åŒ–æˆæ·±åº¦å›¾ å­—èŠ‚æ¨å‡ºçš„æ¨¡å‹ï¼Œæ•ˆæœéå¸¸å¥½ï¼Œå®é™…æ•ˆæœè§è§†é¢‘ã€‚ é¡¹ç›®åœ°å€ï¼šhttps://t.co/QHbp8tPQbC å¤„ç†è§†é¢‘çš„åœ¨çº¿ä½“éªŒï¼šhttps://t.co/ROdrmoIF2G å¤„ç†å›¾ç‰‡çš„åœ¨çº¿ä½“éªŒï¼šhttps://t.co/hnuxjjQoWM Githubï¼šhttps://t.co/76X2Cv7Tb8 https://t.co/mGc3l1wFbs" / X - https://twitter.com/Gorden_Sun/status/1757046907917275608
- #CVPR2024 on X: "CVPR 2007 Best Paper Award Dynamic 3D Scene Analysis from a Moving Vehicles B. Leibe, N. Cornelis, K. Cornelis, L. Van Gool #TBThursday https://t.co/weeChSRYJk" / X - https://twitter.com/CVPR/status/1758129034008154401
- Dreaming Tulpa ğŸ¥“ğŸ‘‘ on X: "TikTok presents Boximator! This method can generate rich and controllable motions for image-to-video generations by drawing box constraints and motion paths onto an image and combining it with a prompt: "A girl in red is covering her face with a skull." 10 crazy examples: https://t.co/Jy3op9BEdB" / X - https://twitter.com/dreamingtulpa/status/1756965825657925967
- Gorden Sun on X: "Depth Anythingï¼šæŠŠå›¾ç‰‡å’Œè§†é¢‘è½¬åŒ–æˆæ·±åº¦å›¾ å­—èŠ‚æ¨å‡ºçš„æ¨¡å‹ï¼Œæ•ˆæœéå¸¸å¥½ï¼Œå®é™…æ•ˆæœè§è§†é¢‘ã€‚ é¡¹ç›®åœ°å€ï¼šhttps://t.co/QHbp8tPQbC å¤„ç†è§†é¢‘çš„åœ¨çº¿ä½“éªŒï¼šhttps://t.co/ROdrmoIF2G å¤„ç†å›¾ç‰‡çš„åœ¨çº¿ä½“éªŒï¼šhttps://t.co/hnuxjjQoWM Githubï¼šhttps://t.co/76X2Cv7Tb8 https://t.co/mGc3l1wFbs" / X - https://twitter.com/Gorden_Sun/status/1757046907917275608?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1757046907917275608%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- #CVPR2024 on X: "CVPR 2007 Best Paper Award Dynamic 3D Scene Analysis from a Moving Vehicles B. Leibe, N. Cornelis, K. Cornelis, L. Van Gool #TBThursday https://t.co/weeChSRYJk" / X - https://twitter.com/CVPR/status/1758129034008154401
- Micah Berkley on X: "I'm very impressed with new Alibaba's Vision AI model. In some way's it's better than GPT4 because it does an extremely impressive job at identifying people. The way that I use these vision models in business is usually to interface images into words, or in my case, I use them https://t.co/mgUvBLQXRe" / X - https://twitter.com/MicahBerkley/status/1754019702320374070
- Bilawal Sidhu on X: "AI can count sheep. But it never needs to go to sleep. It can see everything, everywhere, all at onceâ€¦ at a certain P/R for certain semantic classes ;) ğŸ‘ â˜ï¸ ğŸ‘ â˜ï¸ https://t.co/6f1cXSLEOG" / X - https://twitter.com/bilawalsidhu/status/1755071169047277939
- SkalskiP on X: "REAL-TIME object detection WITHOUT TRAINING YOLO-World is a new SOTA open-vocabulary object detector that outperforms previous models in terms of both accuracy and speed. 35.4 AP with 52.0 FPS on V100. â†“ read more https://t.co/SoqFyEk41V" / X - https://twitter.com/skalskip92/status/1754916529672438173
- Hieu ğŸš€ on X: "This AI helps you go to sleep real fast. Kidding, it counts sheep https://t.co/sYW1R7NWi4" / X - https://twitter.com/hieuSSR/status/1754808953480032546?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1754808953480032546%7Ctwgr%5Efe6b38a58ad6b9fdd603c89da0b01cb57edbdfe9%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fethanbholland.com%2F2024%2F04%2F08%2Fthe-viggle-ai-memes-impact-on-image-to-video-awareness%2F
- Dreaming Tulpa ğŸ¥“ğŸ‘‘ on X: "TikTok presents Boximator! This method can generate rich and controllable motions for image-to-video generations by drawing box constraints and motion paths onto an image and combining it with a prompt: "A girl in red is covering her face with a skull." 10 crazy examples: https://t.co/Jy3op9BEdB" / X - https://x.com/dreamingtulpa/status/1756965825657925967
### April 2023

- [2304.03199] Face Animation with an Attribute-Guided Diffusion Model - https://arxiv.org/abs/2304.03199
- <https://www.catalyzex.com/paper/face-animation-with-an-attribute-guided/code>

## 2022

### March 2022

- [2203.06605] Depth-Aware Generative Adversarial Network for Talking Head Video Generation - https://arxiv.org/abs/2203.06605
- DaGAN - a Hugging Face Space by HarlanHong - https://huggingface.co/spaces/HarlanHong/DaGAN
- DaGAN: Depth-Aware Generative Adversarial Network for Talking Head Video Generation - https://harlanhong.github.io/publications/dagan.html

## 2021

### April 2021

- face-vid2vid - https://nvlabs.github.io/face-vid2vid/
- tcwang0509/TalkingHead-1KH - https://github.com/tcwang0509/TalkingHead-1KH?tab=readme-ov-file
- GitHub - happy-jihye/face-vid2vid-demo - https://github.com/happy-jihye/face-vid2vid-demo

## 2020

### January 2020

- First Order Motion Model for Image Animation - YouTube - https://www.youtube.com/watch?v=u-0cQ-grXBQ

## 2019

### December 2019

- First order model - https://aliaksandrsiarohin.github.io/first-order-model-website/
