# Talking-Head & Embodied-Avatar Research Timeline

**Purpose**  
A continuously updated chronology of research papers, open-source demos, and product releases that advance photorealistic human-animation technologies—voice-driven “talking heads,” volumetric avatars, depth-aware diffusion, and adjacent robotics or embodied-agent work.  
Although I am *not* an engineer, I track this space closely and curate milestones that feel genuinely consequential, aiming to separate signal from noise for practitioners and observers alike.

**Origins**  
The July 2024 **LivePortrait** demo sparked my curiosity about how quickly still-image animation was converging with full-body digital humans. This document began as personal notes; it is now shared publicly so the broader community can trace the field’s evolution and, ideally, contribute their own discoveries.

**Scope**  
Core focus — face animation, lip-sync, depth/segmentation pipelines, Gaussian-splat-based scene representation, audio-to-face, 3D Gaussian avatars, and related visual-speech synthesis.  
* Adjacent threads — embodied robotics and agent embodiment are welcome, though I maintain a richer robotics log separately at <https://ethanbholland.com/category/ai/robotics-embodiment/>.

**Using this timeline**  
Entries appear in **reverse chronological order** (newest first) under dated headings. Each line contains: **Title · link · brief one-liner**. Use your browser’s search to filter by keyword or year.

**Contributing**  
1. Open an **Issue** or **Pull Request** on GitHub.  
2. Add one line per item, formatted as above (date first, then title, then link).  
3. Keep commentary brief—this list is a high-signal reference, not a blog post.

**Acknowledgments**  
Gratitude to **Bilawal Sidhu** and **Jack Saunders** for inspiring this compilation and for their own exemplary curation work. Additional commentary and related essays live at <https://ethanbholland.com/>.

**License**  
Content is released under **CC BY 4.0**. Reuse freely with attribution.

---
